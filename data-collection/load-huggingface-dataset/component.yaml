name: Load HuggingFace Dataset
description: Load a Huggingface Dataset.
inputs:
- {name: path, type: String, description: 'Path from which to load the dataset. Huggingfaces
    hub for datasets is supported. Example: "Lehrig/Monkey-Species-Collection".'}
- {name: configuration, type: String, description: 'Name of the dataset configuration
    to load. Example: "downsized".', default: '', optional: true}
- {name: split, type: String, description: 'Split within the dataset. If None, all
    splits are loaded as a DatasetDict. Example: "train",', optional: true}
- {name: label_columns, type: 'typing.List[str]', description: 'Optional list of label
    column names to be fetched as optional, additional output. Example: ["label"].',
  optional: true}
outputs:
- {name: dataset_dir, type: String, description: 'Target directory where the dataset
    will be loaded to. Should be available as a mount from a PVC. Example: "/blackboard/dataset".'}
- {name: labels, type: 'typing.Dict[str, typing.List[str]]'}
implementation:
  container:
    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def Load_HuggingFace_Dataset(
          path,
          dataset_dir,
          configuration = "",
          split = None,
          label_columns = None,
      ):
          """
          Load a Huggingface Dataset.

                  Parameters:
                          path: Path from which to load the dataset. Huggingfaces hub for datasets is supported. Example: "Lehrig/Monkey-Species-Collection".
                          dataset_dir: Target directory where the dataset will be loaded to. Should be available as a mount from a PVC. Example: "/blackboard/dataset".
                          configuration: Name of the dataset configuration to load. Example: "downsized".
                          split: Split within the dataset. If None, all splits are loaded as a DatasetDict. Example: "train",
                          label_columns: Optional list of label column names to be fetched as optional, additional output. Example: ["label"].
                  Returns:
                          labels: Dictionary mapping label columns to associated labels, if available. Empty dictionary otherwise. Example: {"labels": ["cat", "dog"]}
          """

          from collections import namedtuple
          from datasets import load_dataset
          from datasets.dataset_dict import DatasetDict
          import logging
          import os
          from PIL.Image import Image
          import sys

          logging.basicConfig(
              stream=sys.stdout,
              level=logging.INFO,
              format="%(levelname)s %(asctime)s: %(message)s",
          )

          if not configuration:
              configuration = None
          logging.info(
              f"Loading dataset from '{path}' using configuration '{configuration}'..."
          )
          dataset = load_dataset(path=path, name=configuration, split=split)

          logging.info("Reading image files into bytes...")

          # see: https://huggingface.co/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk
          def read_image_file(example):
              for column in example:
                  if isinstance(example[column], Image):
                      with open(example[column].filename, "rb") as f:
                          example[column] = {"bytes": f.read()}
              return example

          # note: batching in map caused caching issues, so not using it for now
          dataset = dataset.map(read_image_file)

          logging.info(f"Saving dataset to '{dataset_dir}'...")
          if not os.path.exists(dataset_dir):
              os.makedirs(dataset_dir)
          dataset.save_to_disk(dataset_dir)

          logging.info(f"Dataset saved. Contents of '{dataset_dir}':")
          logging.info(os.listdir(dataset_dir))

          labels = dict()
          if label_columns is not None:
              if isinstance(dataset, DatasetDict):
                  dataset = next(iter(dataset.values()))
              for label_column in label_columns:
                  logging.info(f"Fetching labels from column '{label_column}'...")
                  labels[label_column] = dataset.features[label_column].names

          output = namedtuple("LoadDatasetOutput", ["labels"])

          logging.info("Finished.")
          return output(labels)

      def _serialize_json(obj) -> str:
          if isinstance(obj, str):
              return obj
          import json

          def default_serializer(obj):
              if hasattr(obj, 'to_struct'):
                  return obj.to_struct()
              else:
                  raise TypeError(
                      "Object of type '%s' is not JSON serializable and does not have .to_struct() method."
                      % obj.__class__.__name__)

          return json.dumps(obj, default=default_serializer, sort_keys=True)

      import json
      import argparse
      _parser = argparse.ArgumentParser(prog='Load HuggingFace Dataset', description='Load a Huggingface Dataset.')
      _parser.add_argument("--path", dest="path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--configuration", dest="configuration", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--split", dest="split", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--label-columns", dest="label_columns", type=json.loads, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--dataset-dir", dest="dataset_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = Load_HuggingFace_Dataset(**_parsed_args)

      _output_serializers = [
          _serialize_json,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --path
    - {inputValue: path}
    - if:
        cond: {isPresent: configuration}
        then:
        - --configuration
        - {inputValue: configuration}
    - if:
        cond: {isPresent: split}
        then:
        - --split
        - {inputValue: split}
    - if:
        cond: {isPresent: label_columns}
        then:
        - --label-columns
        - {inputValue: label_columns}
    - --dataset-dir
    - {outputPath: dataset_dir}
    - '----output-paths'
    - {outputPath: labels}
