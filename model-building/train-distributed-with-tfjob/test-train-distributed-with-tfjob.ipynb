{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-medication",
   "metadata": {},
   "source": [
    "# Component Test: Train Distributed with TFJob\n",
    "\n",
    "## Author\n",
    "- Sebastian Lehrig <sebastian.lehrig1@ibm.com>\n",
    "\n",
    "## License\n",
    "Apache-2.0 License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-think",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "88fc0794-2483-417b-9872-beaac97ef101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case packages cannot be found, do:\n",
    "# mamba uninstall transformers tokenizers\n",
    "# pip install transformers tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8472735-d399-41fc-bdae-e9116de11b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from minio import Minio\n",
    "import tarfile\n",
    "from transformers import AutoTokenizer\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pacific-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user-example-com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_NAME = \"glue-dataset\"\n",
    "DATASET_DIR = f'./{DATASET_NAME}'\n",
    "DATASET_FILE = f'{DATASET_NAME}.tar.gz'\n",
    "MODEL_NAME = \"glue-model\"\n",
    "MODEL_FILE = f'{MODEL_NAME}.tar.gz'\n",
    "\n",
    "# minio-service-kubeflow.apps.b2s001.pbm.ihost.com\n",
    "MINIO_URL = \"minio-service.kubeflow:9000\"\n",
    "MINIO_USER = \"minio\"\n",
    "MINIO_PASS = \"minio123\"\n",
    "DATASETS_BUCKET = \"datasets\"\n",
    "MODELS_BUCKET = \"models\"\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-bedroom",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Client objects for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "republican-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "WARNING:root:Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "kfp_client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a34102-8130-45a8-abf3-474ad930bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(MINIO_URL,\n",
    "                     access_key=MINIO_USER,\n",
    "                     secret_key=MINIO_PASS,\n",
    "                     secure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657d0ab-2b0b-4bd9-8082-6d97e778de95",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "839cf79d-7abe-475e-aee1-72f291acd7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/home/jovyan/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'mrpc', split='train')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e445a0-a741-4974-8299-682594ef764c",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb18d8ac-556d-49bf-9205-234e1b1b293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86bd364-36e8-486a-abae-c2c75e7fea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5d17c0ddabd64fa2.arrow\n"
     ]
    }
   ],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(\n",
    "        examples['sentence1'],\n",
    "        examples['sentence2'],\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74229811-3a4c-4b77-a8ba-cb491e973706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jovyan/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b946e5745aa6d6a4.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda examples: {'labels': examples['label']},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7beb165-e3a2-4ac1-9ffa-e58a07322bfd",
   "metadata": {},
   "source": [
    "## Save data to disk & tar.gz it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf33e86a-d592-42fa-9a8f-767d5098a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname='.')\n",
    "\n",
    "\n",
    "dataset.save_to_disk(DATASET_DIR)\n",
    "make_tarfile(DATASET_FILE, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec080f79-7b14-404c-be23-57e040f1d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "            x: dataset[x]\n",
    "            for x in ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f7a88-e8cb-40d9-9291-e11218e6dd01",
   "metadata": {},
   "source": [
    "## Upload data to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1dce520-4caa-48ec-8374-4c12e7bf4226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://minio-service.kubeflow:9000/datasets/glue-dataset.tar.gz'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create datasets bucket if it does not yet exist\n",
    "response = minio_client.list_buckets()\n",
    "datasets_bucket_exists = False\n",
    "for bucket in response:\n",
    "    if bucket.name == DATASETS_BUCKET:\n",
    "        datasets_bucket_exists = True\n",
    "\n",
    "if not datasets_bucket_exists:\n",
    "    minio_client.make_bucket(bucket_name=DATASETS_BUCKET)\n",
    "\n",
    "minio_client.fput_object(\n",
    "    bucket_name=DATASETS_BUCKET,  # bucket name in Minio\n",
    "    object_name=DATASET_FILE,  # file name in bucket of Minio\n",
    "    file_path=DATASET_FILE,  # file path / name in local system\n",
    ")\n",
    "\n",
    "# https://kubeflow.apps.b2s001.pbm.ihost.com/pipeline/artifacts/minio/datasets/glue-dataset.tar.gz?namespace=user-example-com\n",
    "s3_address = f\"s3://{MINIO_URL}/{DATASETS_BUCKET}/{DATASET_FILE}\"\n",
    "s3_address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45803c-fe7d-4e03-9e96-138850458d35",
   "metadata": {},
   "source": [
    "## Specify train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da0329f1-0f7e-480d-b8fa-63962dfc6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parameters = {\n",
    "    \"dataset_bucket\": DATASETS_BUCKET,\n",
    "    \"model_bucket\": MODELS_BUCKET,\n",
    "    \"dataset_file\": DATASET_FILE,\n",
    "    \"model_file\": MODEL_FILE,\n",
    "    \"storage_uri\": MINIO_URL,\n",
    "    \"storage_username\": MINIO_USER,\n",
    "    \"storage_password\": MINIO_PASS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad1bfabd-1ee3-4340-897b-09a94efad660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    dataset_bucket: str,\n",
    "    model_bucket: str,\n",
    "    dataset_file: str,\n",
    "    model_file: str,\n",
    "    storage_uri: str,\n",
    "    storage_username: str,\n",
    "    storage_password: str\n",
    "):\n",
    "    \"\"\"See https://github.com/kubeflow/training-operator/tree/master/examples/tensorflow/distribution_strategy/keras-API.\"\"\"\n",
    "\n",
    "    from datasets import load_from_disk\n",
    "    import json\n",
    "    from minio import Minio\n",
    "    import os\n",
    "    import tarfile\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "    def load_data(minio_client):\n",
    "        BUFFER_SIZE = 10000\n",
    "        DATSET_PATH = \"./dataset\"\n",
    "\n",
    "        print(f'Fetching data from {dataset_bucket}/{dataset_file}...')\n",
    "        data = minio_client.get_object(dataset_bucket, dataset_file)\n",
    "        with open(dataset_file, 'wb') as file_data:\n",
    "            for d in data.stream(32*1024):\n",
    "                file_data.write(d)\n",
    "\n",
    "        print(f'Extracting {dataset_file} to {DATSET_PATH}...')\n",
    "        with tarfile.open(dataset_file, \"r:gz\") as tar_gz_ref:\n",
    "            tar_gz_ref.extractall(DATSET_PATH)\n",
    "\n",
    "        print('Result:')\n",
    "        print(os.listdir(DATSET_PATH))\n",
    "\n",
    "        print(f'Loading dataset from {DATSET_PATH}...')\n",
    "        dataset = load_from_disk(DATSET_PATH)\n",
    "\n",
    "        print('Transforming to TensorFlow format...')\n",
    "        dataset.set_format(\n",
    "            type='tensorflow',\n",
    "            columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
    "        )\n",
    "        features = {\n",
    "            x: dataset[x]\n",
    "            for x in ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "        }\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (features, dataset[\"labels\"])\n",
    "        )\n",
    "\n",
    "        return tf_dataset.cache().shuffle(BUFFER_SIZE)\n",
    "\n",
    "    def make_tarfile(output_filename, source_dir):\n",
    "        with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "            tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "    def upload_data(file, minio_client):\n",
    "        print(f'Uploading {file} to {model_bucket}/{file}...')\n",
    "\n",
    "        # Create models bucket if it does not yet exist\n",
    "        response = minio_client.list_buckets()\n",
    "        models_bucket_exists = False\n",
    "        for bucket in response:\n",
    "            if bucket.name == model_bucket:\n",
    "                models_bucket_exists = True\n",
    "\n",
    "        if not models_bucket_exists:\n",
    "            minio_client.make_bucket(bucket_name=model_bucket)\n",
    "\n",
    "        minio_client.fput_object(\n",
    "            bucket_name=model_bucket,  # bucket name in Minio\n",
    "            object_name=file,  # file name in bucket of Minio\n",
    "            file_path=file,  # file path / name in local system\n",
    "        )\n",
    "\n",
    "    def build_and_compile_model():\n",
    "        print('Building model...')\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-cased\"\n",
    "        )\n",
    "        model.summary()\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE,\n",
    "            from_logits=True\n",
    "        )\n",
    "        model.compile(optimizer=opt, loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def decay(epoch):\n",
    "        if epoch < 3:\n",
    "            return 1e-3\n",
    "        if 3 <= epoch < 7:\n",
    "            return 1e-4\n",
    "        return 1e-5\n",
    "\n",
    "    # Prepare distributed training with GPU support\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "    tfds.disable_progress_bar()\n",
    "\n",
    "    # to decide if a worker is chief, get TASK_INDEX in Cluster info\n",
    "    tf_config = json.loads(os.environ.get('TF_CONFIG') or '{}')\n",
    "    print(f'tf_config: {tf_config}')\n",
    "\n",
    "    if (tf_config == json.loads('{}')):\n",
    "        TASK_INDEX = 0\n",
    "    else:\n",
    "        TASK_INDEX = tf_config['task']['index']\n",
    "\n",
    "    def is_chief():\n",
    "        return TASK_INDEX == 0\n",
    "\n",
    "    # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "    # layers on each device across all workers\n",
    "    # if your GPUs don't support NCCL, replace \"communication\" with another\n",
    "    communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "        implementation=tf.distribute.experimental.CommunicationImplementation.RING\n",
    "    )\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "        communication_options=communication_options\n",
    "    )\n",
    "\n",
    "    BATCH_SIZE_PER_REPLICA = 8\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "    print(f'BATCH_SIZE_PER_REPLICA: {BATCH_SIZE_PER_REPLICA}')\n",
    "    print(f'num_replicas_in_sync: {strategy.num_replicas_in_sync}')\n",
    "    print(f'BATCH_SIZE: {BATCH_SIZE}')\n",
    "\n",
    "    with strategy.scope():\n",
    "        minio_client = Minio(\n",
    "            storage_uri,\n",
    "            access_key=storage_username,\n",
    "            secret_key=storage_password,\n",
    "            secure=False\n",
    "        )\n",
    "        ds_train = load_data(minio_client).batch(BATCH_SIZE).repeat()\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_distribute.auto_shard_policy = \\\n",
    "            tf.data.experimental.AutoShardPolicy.DATA\n",
    "        ds_train = ds_train.with_options(options)\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = build_and_compile_model()\n",
    "\n",
    "    # Checkpointing\n",
    "    checkpoint_dir = \"/train/checkpoint\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "    # Function for decaying the learning rate.\n",
    "    # You can define any decay function you need.\n",
    "    # Callback for printing the LR at the end of each epoch.\n",
    "    class PrintLR(tf.keras.callbacks.Callback):\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print('\\nLearning rate for epoch {} is {}'.format(\n",
    "                epoch + 1, multi_worker_model.optimizer.lr.numpy()))\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_prefix,\n",
    "            save_weights_only=True\n",
    "        ),\n",
    "        tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "        PrintLR()\n",
    "    ]\n",
    "\n",
    "    # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "    # number of steps per epoch. Note that the numbers here are for demonstration\n",
    "    # purposes only and may not sufficiently produce a model with good quality.\n",
    "    print('Training model...')\n",
    "    multi_worker_model.fit(\n",
    "        ds_train,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=70,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Saving a model\n",
    "    saved_model_dir = \"/train/saved_model/\"\n",
    "    if is_chief():\n",
    "        model_path = saved_model_dir\n",
    "\n",
    "    else:\n",
    "        # Save to a path that is unique across workers.\n",
    "        model_path = saved_model_dir + '/worker_tmp_' + str(TASK_INDEX)\n",
    "\n",
    "    multi_worker_model.save(model_path)\n",
    "\n",
    "    # Upload to object store\n",
    "    if is_chief():\n",
    "        make_tarfile(model_file, model_path)\n",
    "        upload_data(model_file, minio_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "419c2e39-796f-4cec-a0cc-f9c85a5e6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_comp_text = kfp.components.func_to_component_text(\n",
    "    func=train_model,\n",
    "    packages_to_install=[\"transformers\", \"tf_utils\", \"tensorflow_datasets\"],\n",
    "    base_image='quay.io/ibm/kubeflow-notebook-image-ppc64le@sha256:97695b7b4dfab12a65b3d9aaea65649bee1769e578c0965f96648aa55f81fb27'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0083dbb8-88c0-4a7b-b052-521cbfdbf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_comp_yaml = yaml.safe_load(train_model_comp_text)\n",
    "container_yaml = train_model_comp_yaml[\"implementation\"][\"container\"]\n",
    "\n",
    "image = container_yaml[\"image\"]\n",
    "command = container_yaml[\"command\"]\n",
    "args = container_yaml[\"args\"]\n",
    "for idx, arg in enumerate(args):\n",
    "    if type(arg) is dict:\n",
    "        args[idx] = train_parameters[arg[\"inputValue\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974913f-6ea7-4c53-832c-8ac7b729f27f",
   "metadata": {},
   "source": [
    "## Load component from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "840fecc8-afb1-46e6-ba35-547f0ede9d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distributed_train_comp = kfp.components.load_component_from_file(\n",
    "    \"component.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-suicide",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "material-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Component Test - Train Distributed with TFJob',\n",
    "  description='A simple component test'\n",
    ")\n",
    "def train_pipeline(\n",
    "    model_name: str,\n",
    "    image: str,\n",
    "    command: list,\n",
    "    args: list,\n",
    "    namespace: str,\n",
    "):\n",
    "\n",
    "    distributed_train_comp(\n",
    "        model_name=model_name,\n",
    "        image=image,\n",
    "        command=command,\n",
    "        args=args,\n",
    "        namespace=namespace,\n",
    "        number_of_workers=\"2\",\n",
    "        pvc_size=\"10Gi\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3bec0-144a-49d4-9c92-af5b84442231",
   "metadata": {},
   "source": [
    "## Run the pipline within an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a2cf14a-9d24-4919-ab2c-f93f99b5340d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/ca9d4882-4304-416f-8736-3e53fb4babbc\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/275c06ac-7ed5-4618-b041-b9dc707df7f3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=275c06ac-7ed5-4618-b041-b9dc707df7f3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify argument values for your pipeline run.\n",
    "arguments = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'image': image,\n",
    "    'command': command,\n",
    "    'args': args,\n",
    "    'namespace': NAMESPACE,\n",
    "}\n",
    "\n",
    "kfp_client.create_run_from_pipeline_func(\n",
    "    train_pipeline,\n",
    "    arguments=arguments,\n",
    "    namespace=NAMESPACE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98076f33-e245-49eb-ac99-b37187e5d78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
