{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f20b995-81e6-46ae-952a-f36267ee6cea",
   "metadata": {},
   "source": [
    "# Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266a528d-a77c-460d-8b4c-3632170c702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "from kfp.components import create_component_from_func, InputPath, OutputPath\n",
    "from typing import Dict, List\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "\n",
    "def Plot_Confusion_Matrix(\n",
    "    input_columns: List[str],\n",
    "    label_columns: Dict[str, List[str]],\n",
    "    test_dataset_dir: InputPath(str),\n",
    "    model_dir: InputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(),\n",
    "    dataset_split: str = \"test\",\n",
    "    batch_size: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix based on a Huggingface Dataset with a test split and a model trained via Keras.\n",
    "\n",
    "            Parameters:\n",
    "                    input_columns: Input columns for the model. Examples: [\"mel_spectrogram\", \"pixel_values\"].\n",
    "                    label_columns: Dictionary mapping each label column to a list of possible labels. Example: {\"genre\": [\"Blues\", \"Rock\", \"Country\"]}\n",
    "                    test_dataset_dir: Directory where to load test data from. Example: \"/blackboard/prep_dataset\".\n",
    "                    model_dir: Directory where to load the model from. Example: \"/blackboard/model\".\n",
    "                    dataset_split: Optional name of a dataset's split. Defaults to \"test\".\n",
    "                    batch_size: Optional batch size when processing the input dataset. Example: 20.\n",
    "            Returns:\n",
    "                    mlpipeline_ui_metadata_path: Data to plot a confusion matrix. The plotted confusion matrix can be viewed via Kubeflow UI's Vizualization for this component inside a pipeline run.\n",
    "    \"\"\"\n",
    "    from collections.abc import Iterable\n",
    "    import json\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    import tensorflow as tf\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    test_dataset = tf.data.experimental.load(test_dataset_dir)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "    # see: https://github.com/huggingface/datasets/issues/4772\n",
    "    if \"labels\" in label_columns:\n",
    "        label_columns[\"label\"] = label_columns.pop(\"labels\")\n",
    "\n",
    "    def ensure_encoding(label_tensor):\n",
    "        rank = label_tensor.shape.rank\n",
    "\n",
    "        if rank != 1 and rank != 2:\n",
    "            err = f\"Rank of label tensor has to be 1 or 2 but found rank {rank}!\"\n",
    "            logger.error(err)\n",
    "            raise Exception(err)\n",
    "\n",
    "        # transform one-hot vector into integer\n",
    "        return tf.math.argmax(label_tensor, axis=rank - 1)\n",
    "\n",
    "    def prediction_to_encoded_tensor(prediction):\n",
    "        result = None\n",
    "        if isinstance(prediction, np.ndarray):\n",
    "            if len(prediction) == 1:\n",
    "                pred = prediction[0]\n",
    "                if isinstance(pred, np.ndarray):\n",
    "                    if len(pred) == 1:\n",
    "                        result = round(pred[0])\n",
    "                    elif len(pred) > 1:\n",
    "                        result.append(np.argmax(pred, axis=0))\n",
    "                    else:\n",
    "                        err = f\"Unsupport prediction array size: {len(pred[0])}\"\n",
    "                        logger.error(err)\n",
    "                        raise Exception(err)\n",
    "                elif isinstance(pred, (np.floating, float)):\n",
    "                    result.append(round(pred))\n",
    "                elif isinstance(pred, (np.integer, int)):\n",
    "                    result.append(pred)\n",
    "                else:\n",
    "                    err = f\"Unsupport prediction type: {type(pred)}\"\n",
    "                    logger.error(err)\n",
    "                    raise Exception(err)\n",
    "            elif len(prediction) > 1:\n",
    "                result = np.argmax(prediction, axis=0)\n",
    "            else:\n",
    "                err = f\"Unsupport prediction length: {len(prediction)}\"\n",
    "                logger.error(err)\n",
    "                raise Exception(err)\n",
    "        elif isinstance(prediction, tf.Tensor):\n",
    "            result = ensure_encoding(prediction)\n",
    "        elif isinstance(prediction, (np.floating, float)):\n",
    "            result = round(prediction)\n",
    "        elif isinstance(prediction, (np.integer, int)):\n",
    "            result = prediction\n",
    "        else:\n",
    "            err = f\"Unsupport model prediction type: {type(prediction)}\"\n",
    "            logger.error(err)\n",
    "            raise Exception(err)\n",
    "\n",
    "        if isinstance(result, tf.Tensor):\n",
    "            if len(result.shape) == 0:\n",
    "                result = [result]\n",
    "        elif isinstance(result, dict) or isinstance(result, list):\n",
    "            if len(result) == 0:\n",
    "                result = [result]\n",
    "        elif isinstance(result, (np.integer, int)):\n",
    "            result = [result]\n",
    "        else:\n",
    "            err = f\"Unsupport result type: {len(result)}\"\n",
    "            logger.error(err)\n",
    "            raise Exception(err)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def merge_to_dict(dictionary, key, tensor):\n",
    "        if key in dictionary:\n",
    "            dictionary[key] = tf.concat([dictionary.pop(key), tensor], axis=0)\n",
    "        else:\n",
    "            dictionary[key] = tensor\n",
    "\n",
    "    def process_tensors(dictionary, tensors, label_columns, index2label):\n",
    "        if isinstance(tensors, dict):\n",
    "            # Multi-label support\n",
    "            for key, tensor in tensors.items():\n",
    "                encoded_tensor = prediction_to_encoded_tensor(tensor)\n",
    "                merge_to_dict(dictionary, key, encoded_tensor)\n",
    "        elif isinstance(tensors, list) or isinstance(tensors, np.ndarray):\n",
    "            # Multi-label support; assume order & get name from dataset\n",
    "            for idx, tensor in enumerate(tensors):\n",
    "                encoded_tensor = prediction_to_encoded_tensor(tensor)\n",
    "                if len(index2label) == 1:\n",
    "                    label = index2label[0]\n",
    "                else:\n",
    "                    label = index2label[idx]\n",
    "                merge_to_dict(dictionary, label, encoded_tensor)\n",
    "        elif isinstance(tensors, tf.Tensor):\n",
    "            # Assuming single label\n",
    "            if len(label_columns) > 1:\n",
    "                err = f\"Model provides only 1 output but got {len(label_columns)} label columns!\"\n",
    "                logger.error(err)\n",
    "                raise Exception(err)\n",
    "            key = next(iter(label_columns))\n",
    "\n",
    "            for tensor in tensors:\n",
    "                encoded_tensor = prediction_to_encoded_tensor(tensor)\n",
    "                merge_to_dict(dictionary, key, encoded_tensor)\n",
    "        else:\n",
    "            err = f\"Unsupported tensors type: {type(tensors)}!\"\n",
    "            logger.error(err)\n",
    "            raise Exception(err)\n",
    "\n",
    "    # Get label indexes & names from dataset\n",
    "    index2label = dict()\n",
    "    label_specs = test_dataset.element_spec[1]\n",
    "    if isinstance(label_specs, Iterable):\n",
    "        for i, label in enumerate(label_specs):\n",
    "            index2label[i] = label\n",
    "    else:\n",
    "        # Fall-back to Huggingface dataset\n",
    "        for i, label in enumerate(label_columns):\n",
    "            index2label[i] = label\n",
    "    logging.info(f\"Indexes mapped to labels: {index2label}\")\n",
    "\n",
    "    # each dataset_batch is of size batch_size; results are aggregate inside this loop\n",
    "    y_true = dict()\n",
    "    y_pred = dict()\n",
    "    for dataset_batch in test_dataset:\n",
    "        label_tensors = dataset_batch[1]\n",
    "        process_tensors(y_true, label_tensors, label_columns, index2label)\n",
    "\n",
    "        feature_tensors = dataset_batch[0]\n",
    "        predictions = model.predict(feature_tensors)\n",
    "        process_tensors(y_pred, predictions, label_columns, index2label)\n",
    "\n",
    "    logging.info(f\"Final labels: {y_true}\")\n",
    "    logging.info(f\"Final predictions: {y_pred}\")\n",
    "\n",
    "    confusion_matrices = []\n",
    "    for label_column, labels in label_columns.items():\n",
    "        confusion_matrix = tf.math.confusion_matrix(\n",
    "            labels=y_true[label_column],\n",
    "            predictions=y_pred[label_column],\n",
    "            num_classes=len(labels),\n",
    "        )\n",
    "\n",
    "        data = []\n",
    "        for target_index, target_row in enumerate(confusion_matrix):\n",
    "            for predicted_index, count in enumerate(target_row):\n",
    "                data.append(\n",
    "                    (labels[target_index], labels[predicted_index], count.numpy())\n",
    "                )\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"target\", \"predicted\", \"count\"])\n",
    "\n",
    "        confusion_matrices.append(\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"count\", \"type\": \"NUMBER\"},\n",
    "                ],\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": df.to_csv(\n",
    "                    columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n",
    "                ),\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metadata = {\"outputs\": confusion_matrices}\n",
    "\n",
    "    logger.info(\"Dumping mlpipeline_ui_metadata...\")\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "\n",
    "\n",
    "load_dataset_comp = create_component_from_func(\n",
    "    func=Plot_Confusion_Matrix,\n",
    "    output_component_file=\"component.yaml\",\n",
    "    base_image=BASE_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9976b3f-3476-4bf7-ad7a-f4d768d8f87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
