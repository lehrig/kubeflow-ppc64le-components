{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ca7b118-23f4-401e-bfd4-b590e481bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "from kfp.components import create_component_from_func, InputPath, OutputPath\n",
    "from typing import Dict, List\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    input_columns: List[str],\n",
    "    label_columns: Dict[str, List[str]],\n",
    "    test_dataset_dir: InputPath(str),\n",
    "    model_dir: InputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(),\n",
    "    dataset_split: str = \"test\",\n",
    "    num_predictions: int = 0,\n",
    "    batch_size: int = 20,\n",
    "    seq_len: int = 7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix based on the test split of a Huggingface, Numpy or Pandas CSV Dataset and a model trained via Keras or ScikitLearn.\n",
    "\n",
    "            Parameters:\n",
    "                    input_columns: Input columns for the model. Examples: [\"mel_spectrogram\", \"pixel_values\"].\n",
    "                    label_columns: Dictionary mapping each label column to a list of possible labels. Example: {\"genre\": [\"Blues\", \"Rock\", \"Country\"]}\n",
    "                    test_dataset_dir: Directory where to load test data from. Example: \"/blackboard/prep_dataset\".\n",
    "                    model_dir: Directory where to load the model from. Example: \"/blackboard/model\".\n",
    "                    dataset_split: Optional name of a dataset's split. Defaults to \"test\".\n",
    "                    batch_size: Optional batch size when processing the input dataset. Example: 20.\n",
    "                    num_predictions: A number restricting how many predictions should be made. Default value 0 indicates that the whole test dataset should be used\n",
    "                    seq_len: Sequence length for time series datasets. Defaults to 7.\n",
    "            Returns:\n",
    "                    mlpipeline_ui_metadata_path: Data to plot a confusion matrix. The plotted confusion matrix can be viewed via Kubeflow UI's Vizualization for this component inside a pipeline run.\n",
    "    \"\"\"\n",
    "    from collections.abc import Iterable\n",
    "    import json\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    import os\n",
    "    import joblib\n",
    "    import lightgbm\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    def relabel_map(item, label):\n",
    "        label = [label]\n",
    "        return item, label\n",
    "\n",
    "    files = os.listdir(test_dataset_dir)\n",
    "    csv = list(filter(lambda x: \".csv\" in x, files))\n",
    "    npz = list(filter(lambda x: \".npz\" in x, files))\n",
    "\n",
    "    # pandas dataset\n",
    "    if len(csv) > 0:\n",
    "        path = os.path.join(test_dataset_dir, csv[0])\n",
    "        lc = list(label_columns.keys())\n",
    "        test_dataset = tf.data.experimental.make_csv_dataset(\n",
    "            path,\n",
    "            batch_size=1,\n",
    "            label_name=lc[0],\n",
    "            select_columns=input_columns + lc,\n",
    "        )\n",
    "        test_dataset = test_dataset.map(relabel_map)\n",
    "        test_dataset = test_dataset.batch(batch_size)\n",
    "        logger.info(\"Loaded CSV dataset.\")\n",
    "    # numpy dataset\n",
    "    elif len(npz) > 0:\n",
    "        data = np.load(os.path.join(test_dataset_dir, npz[0]))\n",
    "        x, y = data[\"x\"], data[\"y\"]\n",
    "        test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "            x, y, sequence_length=seq_len, batch_size=batch_size\n",
    "        )\n",
    "        logger.info(\"Loaded numpy timeseries dataset.\")\n",
    "    # plain tensorflow dataset\n",
    "    else:\n",
    "        test_dataset = tf.data.experimental.load(test_dataset_dir)\n",
    "        logger.info(\"Loaded TF dataset.\")\n",
    "\n",
    "    if num_predictions == 0:\n",
    "        try:\n",
    "            num_predictions = test_dataset.__len__()\n",
    "        except Exception as e:\n",
    "            logger.warning(e)\n",
    "            num_predictions = test_dataset.cardinality().numpy()\n",
    "\n",
    "    tff, pkl = [], []\n",
    "    if os.path.isdir(model_dir):\n",
    "        models = os.listdir(model_dir)\n",
    "        tff = list(filter(lambda x: \".pb\" in x, models))\n",
    "        pkl = list(filter(lambda x: \".pkl\" in x, models))\n",
    "\n",
    "    # keras/tf model\n",
    "    if len(tff) > 0:\n",
    "        model = keras.models.load_model(model_dir)\n",
    "    # sklearn model\n",
    "    elif len(pkl) > 0:\n",
    "        model = joblib.load(os.path.join(model_dir, pkl[0]))\n",
    "    # lightgbm/sklearn/otherwise pickled model\n",
    "    else:\n",
    "        try:\n",
    "            model = joblib.load(model_dir)\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Model format not yet supported:\", e)\n",
    "\n",
    "    # see: https://github.com/huggingface/datasets/issues/4772\n",
    "    if \"labels\" in label_columns:\n",
    "        label_columns[\"label\"] = label_columns.pop(\"labels\")\n",
    "\n",
    "    def ensure_encoding(label_tensor):\n",
    "        rank = label_tensor.shape.rank\n",
    "\n",
    "        if rank != 1 and rank != 2:\n",
    "            err = f\"Rank of label tensor has to be 1 or 2 but found rank {rank}!\"\n",
    "            raise Exception(err)\n",
    "\n",
    "        # transform one-hot vector into integer\n",
    "        return tf.math.argmax(label_tensor, axis=rank - 1)\n",
    "\n",
    "    def prediction_to_encoded_tensor(prediction):\n",
    "        result = []\n",
    "        if isinstance(prediction, np.ndarray):\n",
    "            if len(prediction) == 1:\n",
    "                pred = prediction[0]\n",
    "                if isinstance(pred, np.ndarray):\n",
    "                    if len(pred) == 1:\n",
    "                        result = round(pred[0])\n",
    "                    elif len(pred) > 1:\n",
    "                        result.append(np.argmax(pred, axis=0))\n",
    "                    else:\n",
    "                        err = f\"Unsupport prediction array size: {len(pred[0])}\"\n",
    "                        raise Exception(err)\n",
    "                elif isinstance(pred, (np.floating, float)):\n",
    "                    result.append(round(pred))\n",
    "                elif isinstance(pred, (np.integer, int)):\n",
    "                    result.append(pred)\n",
    "                else:\n",
    "                    err = f\"Unsupport prediction type: {type(pred)}\"\n",
    "                    raise Exception(err)\n",
    "            elif len(prediction) > 1:\n",
    "                result = np.argmax(prediction, axis=0)\n",
    "            else:\n",
    "                err = f\"Unsupport prediction length: {len(prediction)}\"\n",
    "                raise Exception(err)\n",
    "        elif isinstance(prediction, tf.Tensor):\n",
    "            result = ensure_encoding(prediction)\n",
    "        elif isinstance(prediction, (np.floating, float)):\n",
    "            result = round(prediction)\n",
    "        elif isinstance(prediction, (np.integer, int)):\n",
    "            result = prediction\n",
    "        else:\n",
    "            err = f\"Unsupport model prediction type: {type(prediction)}\"\n",
    "            raise Exception(err)\n",
    "\n",
    "        if isinstance(result, tf.Tensor):\n",
    "            if len(result.shape) == 0:\n",
    "                result = [result]\n",
    "        elif isinstance(result, dict) or isinstance(result, list):\n",
    "            if len(result) == 0:\n",
    "                result = [result]\n",
    "        elif isinstance(result, (np.integer, int)):\n",
    "            result = [result]\n",
    "        else:\n",
    "            err = f\"Unsupport result type: {len(result)}\"\n",
    "            raise Exception(err)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def merge_to_dict(dictionary, key, tensor):\n",
    "        if key in dictionary:\n",
    "            dictionary[key] = tf.concat([dictionary.pop(key), tensor], axis=0)\n",
    "        else:\n",
    "            dictionary[key] = tensor\n",
    "\n",
    "    def process_tensors(dictionary, tensors, label_columns, index2label):\n",
    "        if isinstance(tensors, dict):\n",
    "            # Multi-label support\n",
    "            for key, tensor in tensors.items():\n",
    "                encoded_tensor = prediction_to_encoded_tensor(tensor)\n",
    "                merge_to_dict(dictionary, key, encoded_tensor)\n",
    "        elif isinstance(tensors, list) or isinstance(tensors, np.ndarray):\n",
    "            # Multi-label support; assume order & get name from dataset\n",
    "            for idx, tensor in enumerate(tensors):\n",
    "                encoded_tensor = prediction_to_encoded_tensor(tensor)\n",
    "                if len(index2label) == 1:\n",
    "                    label = index2label[0]\n",
    "                else:\n",
    "                    label = index2label[idx]\n",
    "                merge_to_dict(dictionary, label, encoded_tensor)\n",
    "        elif isinstance(tensors, tf.Tensor):\n",
    "            # Assuming single label\n",
    "            if len(label_columns) > 1:\n",
    "                err = f\"Model provides only 1 output but got {len(label_columns)} label columns!\"\n",
    "                raise Exception(err)\n",
    "            key = next(iter(label_columns))\n",
    "\n",
    "            for tensor in tensors:\n",
    "                encoded_tensor = prediction_to_encoded_tensor(tensor)\n",
    "                merge_to_dict(dictionary, key, encoded_tensor)\n",
    "        else:\n",
    "            err = f\"Unsupported tensors type: {type(tensors)}!\"\n",
    "            raise Exception(err)\n",
    "\n",
    "    # Get label indexes & names from dataset\n",
    "\n",
    "    index2label = dict()\n",
    "    label_specs = test_dataset.element_spec[1]\n",
    "    if isinstance(label_specs, Iterable):\n",
    "        for i, label in enumerate(label_specs):\n",
    "            index2label[i] = label\n",
    "    else:\n",
    "        # Fall-back to Huggingface dataset\n",
    "        for i, label in enumerate(label_columns):\n",
    "            index2label[i] = label\n",
    "    logging.info(f\"Indexes mapped to labels: {index2label}\")\n",
    "\n",
    "    # each dataset_batch is of size batch_size; results are aggregate inside this loop\n",
    "    y_true = dict()\n",
    "    y_pred = dict()\n",
    "    for dataset_batch in test_dataset.take(num_predictions):\n",
    "        label_tensors = dataset_batch[1]\n",
    "        process_tensors(y_true, label_tensors, label_columns, index2label)\n",
    "\n",
    "        feature_tensors = dataset_batch[0]\n",
    "        try:\n",
    "            # keras/tf model\n",
    "            predictions = model.predict(feature_tensors)\n",
    "        except ValueError:\n",
    "            # lightgbm/sklearn model trained with numpy arrays\n",
    "            feature_tensors = np.array(list(feature_tensors.values())).T[\n",
    "                0\n",
    "            ]  # .reshape(1, -1)\n",
    "            predictions = model.predict(feature_tensors)\n",
    "        process_tensors(y_pred, predictions, label_columns, index2label)\n",
    "\n",
    "    logging.info(f\"Final labels: {y_true}\")\n",
    "    logging.info(f\"Final predictions: {y_pred}\")\n",
    "\n",
    "    confusion_matrices = []\n",
    "    for label_column, labels in label_columns.items():\n",
    "        confusion_matrix = tf.math.confusion_matrix(\n",
    "            labels=y_true[label_column],\n",
    "            predictions=y_pred[label_column],\n",
    "            num_classes=len(labels),\n",
    "        )\n",
    "\n",
    "        data = []\n",
    "        for target_index, target_row in enumerate(confusion_matrix):\n",
    "            for predicted_index, count in enumerate(target_row):\n",
    "                data.append(\n",
    "                    (labels[target_index], labels[predicted_index], count.numpy())\n",
    "                )\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"target\", \"predicted\", \"count\"])\n",
    "\n",
    "        confusion_matrices.append(\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {\"name\": \"target\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n",
    "                    {\"name\": \"count\", \"type\": \"NUMBER\"},\n",
    "                ],\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": df.to_csv(\n",
    "                    columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n",
    "                ),\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metadata = {\"outputs\": confusion_matrices}\n",
    "\n",
    "    logger.info(\"Dumping mlpipeline_ui_metadata...\")\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "\n",
    "\n",
    "load_dataset_comp = create_component_from_func(\n",
    "    func=plot_confusion_matrix,\n",
    "    output_component_file=\"component.yaml\",\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"lightgbm\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c2181-097e-4446-b57d-8f9f88234e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
