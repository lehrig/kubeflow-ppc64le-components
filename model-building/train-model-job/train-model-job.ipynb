{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f20b995-81e6-46ae-952a-f36267ee6cea",
   "metadata": {},
   "source": [
    "# Train Model Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a528d-a77c-460d-8b4c-3632170c702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "from kfp.components import create_component_from_func, InputPath, OutputPath\n",
    "from typing import Dict\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "\n",
    "def Train_Model_Job(\n",
    "    train_dataset_dir: InputPath(str),\n",
    "    validation_dataset_dir: InputPath(str),\n",
    "    train_specification: str,\n",
    "    train_parameters: Dict[str, str],\n",
    "    model_dir: OutputPath(str),\n",
    "    train_mount: str = \"/train\",\n",
    "    model_name: str = \"my-model\",\n",
    "    base_image: str = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\",\n",
    "    node_selector: str = \"\",\n",
    "    pvc_name: str = \"\",\n",
    "    pvc_size: str = \"10Gi\",\n",
    "    cpus: str = \"\",\n",
    "    gpus: int = 0,\n",
    "    memory: str = \"\",\n",
    "    tensorboard_s3_address: str = \"\",\n",
    "    cluster_configuration_secret: str = \"\",\n",
    "    distribution_specification: Dict[str, str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a model. Once trained, the model is persisted to model_dir.\n",
    "\n",
    "            Parameters:\n",
    "                    train_dataset_dir: Path to the directory with training data.\n",
    "                    validation_dataset_dir: Path to the directory with validation data to be used during training.\n",
    "                    train_specification: Training command as generated from a Python function using kfp.components.func_to_component_text.\n",
    "                    train_parameters: Dictionary mapping formal to actual parameters for the training spacification.\n",
    "                    model_dir: Target path where the model will be stored.\n",
    "                    train_mount: Optional mounting point for training data of an existing PVC. Example: \"/train\".\n",
    "                    model_name: Optional name of the model. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.\n",
    "                    base_image: Optional base image for model training. Example: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest.\n",
    "                    node_selector: Optional node selector for worker nodes. Example: nvidia.com/gpu.product: \"Tesla-V100-SXM2-32GB\".\n",
    "                    pvc_name: Optional name to an existing persistent volume claim (pvc). If given, this pvc is mounted into the training job. Example: \"music-genre-classification-j4ssf-training-pvc\".\n",
    "                    pvc_size: Optional size of the storage during model training. Storage is mounted into to the Job based on a persitent volume claim of the given size. Example: 10Gi.\n",
    "                    cpus: Optional CPU limit for the job. Leave empty for cluster defaults (typically no limit). Example: \"1000m\".\n",
    "                    gpus: Optional number of GPUs for the job. Example: 2.\n",
    "                    memory: Optional memory limit for the job. Leave empty for cluster defaults (typically no limit). Example: \"1Gi\".\n",
    "                    tensorboard_s3_address: Optional s3 address where Tensorboard logs shall be stored. Example: \"s3://mlpipeline/tensorboard/my-train-job\".\n",
    "                    cluster_configuration_secret: Optional secret name configuring a (remote) Kubernetes cluster to run the job in and the backing MinIO object store. All secret's data values are optional and appropriate defaults are chosen if not present. The secret may provide a suitable kubernetes bearer token, the associated namespace, a host, etc. Example: \"remote-power-cluster\".\n",
    "                    distribution_specification: Optional dictionary specifiying the distribution behavior. By default, no distributed training is executed, which results in an ordinary Kubernetes Job  for training. Otherwise, dictionary entries determine the distribution behavior. The \"distribution_type\" entry determines the distribution type: \"Job\" (no distribution; ordinary Kubernetes job), \"MPI\" (all-reduce style distribution via Horovod), or \"TF\" (parameter-server style distribution via distributed training with TensorFlow). Depending on the distribution type, additional dictionary entries can be processed. For distributed training jobs, the \"number_of_workers\" (e.g., 2) determines the number of worker replicas for training. Individual resource limits can be controlled via \"worker_cpus\" (e.g., \"1000m\") and \"worker_memory\" (e.g., \"1Gi\"). MPI additionally provides a fine-grained control of launcher cpu and memory limits via \"launcher_cpus\" (e.g., \"1000m\") and \"launcher_memory\" (e.g., \"1Gi\"). Full example with MPI: {\"distribution_type\": \"MPI\", \"number_of_workers\": 2, \"worker_cpus\": \"8\", \"worker_memory\": \"32Gi\", \"launcher_cpus\": \"2\", \"launcher_memory\": \"8Gi\"}\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import errno\n",
    "    import json\n",
    "    import kfp\n",
    "    from kubernetes import client, config, utils, watch\n",
    "    import logging\n",
    "    import os\n",
    "    import shutil\n",
    "    import sys\n",
    "    import yaml\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    ###########################################################################\n",
    "    # Helper Functions\n",
    "    ###########################################################################\n",
    "\n",
    "    def establish_local_cluster_connection():\n",
    "        config.load_incluster_config()\n",
    "        return client.ApiClient()\n",
    "\n",
    "    def get_cluster_configuration(api_client, cluster_configuration_secret):\n",
    "        import base64\n",
    "        from kubernetes.client.rest import ApiException\n",
    "\n",
    "        def decode(secret, key):\n",
    "            data = secret.data[key]\n",
    "            decoded_data = base64.b64decode(data)\n",
    "            return decoded_data.decode(\"utf-8\")\n",
    "\n",
    "        def update_with_secret(secret, dictionary):\n",
    "            for key in dictionary:\n",
    "                if key in secret.data:\n",
    "                    dictionary[key] = decode(secret, key)\n",
    "\n",
    "        cluster_configuration = {\n",
    "            \"access-mode\": \"ReadWriteMany\",\n",
    "            \"minio-accesskey\": \"minio\",\n",
    "            \"minio-bucket\": \"mlpipeline\",\n",
    "            \"minio-job-folder\": \"jobs\",\n",
    "            \"minio-secretkey\": \"minio123\",\n",
    "            \"minio-url\": \"http://minio-service.kubeflow:9000\",\n",
    "            \"remote-host\": \"\",\n",
    "            \"remote-namespace\": \"\",\n",
    "            \"remote-token\": \"\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            default_minio_secret = client.CoreV1Api(api_client).read_namespaced_secret(\n",
    "                \"mlpipeline-minio-artifact\", get_current_namespace()\n",
    "            )\n",
    "\n",
    "            if default_minio_secret.data is None:\n",
    "                logger.info(\n",
    "                    \"MinIO secret (mlpipeline-minio-artifact) includes no data - progressing with default values.\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"Found default MinIO secret (mlpipeline-minio-artifact) - updating cluster configuration accordingly.\"\n",
    "                )\n",
    "                cluster_configuration[\"minio-accesskey\"] = decode(\n",
    "                    default_minio_secret, \"accesskey\"\n",
    "                )\n",
    "                cluster_configuration[\"minio-secretkey\"] = decode(\n",
    "                    default_minio_secret, \"secretkey\"\n",
    "                )\n",
    "        except ApiException as e:\n",
    "            if e.status == 404:\n",
    "                logger.info(\n",
    "                    \"Found no default MinIO secret (mlpipeline-minio-artifact) - progressing with default values.\"\n",
    "                )\n",
    "\n",
    "        if cluster_configuration_secret == \"\":\n",
    "            logger.info(\n",
    "                \"No cluster configuration secret specified - progressing with default values.\"\n",
    "            )\n",
    "            return cluster_configuration\n",
    "\n",
    "        try:\n",
    "            secret = client.CoreV1Api(api_client).read_namespaced_secret(\n",
    "                cluster_configuration_secret, get_current_namespace()\n",
    "            )\n",
    "            if secret.data is None:\n",
    "                logger.info(\n",
    "                    f\"Cluster configuration secret ({cluster_configuration_secret}) includes no data - progressing with default values.\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"Found cluster configuration secret ({cluster_configuration_secret}) - updating cluster configuration accordingly.\"\n",
    "                )\n",
    "                update_with_secret(secret, cluster_configuration)\n",
    "        except ApiException as e:\n",
    "            if e.status == 404:\n",
    "                logger.info(\n",
    "                    f\"Found no cluster configuration secret ({cluster_configuration_secret}) - progressing with default values.\"\n",
    "                )\n",
    "\n",
    "        return cluster_configuration\n",
    "\n",
    "    def establish_training_cluster_connection(local_api_client, cluster_configuration):\n",
    "        is_remote = False\n",
    "        if (\n",
    "            cluster_configuration[\"remote-host\"] == \"\"\n",
    "            or cluster_configuration[\"remote-token\"] == \"\"\n",
    "        ):\n",
    "            logger.info(\n",
    "                \"Remote cluster not configured. Using in-cluster configuration...\"\n",
    "            )\n",
    "            logger.info(\n",
    "                \"Note: assign the name of a secret to the 'cluster_configuration_secret' pipeline argument and add the secret to your cluster.\"\n",
    "            )\n",
    "            logger.info(\"Example secret:\")\n",
    "            logger.info(\"---\")\n",
    "            logger.info(\"apiVersion: v1\")\n",
    "            logger.info(\"kind: Secret\")\n",
    "            logger.info(\"metadata:\")\n",
    "            logger.info(\"  name: my-remote-cluster\")\n",
    "            logger.info(\"stringData:\")\n",
    "            logger.info(\"  access-mode: ReadWriteOnce\")\n",
    "            logger.info(\"  minio-accesskey: minio\")\n",
    "            logger.info(\"  minio-bucket: mlpipeline\")\n",
    "            logger.info(\"  minio-job-folder: jobs\")\n",
    "            logger.info(\"  minio-secretkey: minio123\")\n",
    "            logger.info(\"  minio-url: http://minio-service.kubeflow:9000\")\n",
    "            logger.info(\n",
    "                \"  remote-host: https://istio-ingressgateway-istio-system.apps.mydomain.ai:6443\"\n",
    "            )\n",
    "            logger.info(\"  remote-namespace: default\")\n",
    "            logger.info(\"  remote-token: eyJh...\")\n",
    "            logger.info(\"---\")\n",
    "            logger.info(\n",
    "                \"Where you get the remote-token from your remote cluster as described here:\"\n",
    "            )\n",
    "            logger.info(\n",
    "                \"https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#without-kubectl-proxy\"\n",
    "            )\n",
    "\n",
    "            api_client = local_api_client\n",
    "            if not os.path.exists(train_mount):\n",
    "                logger.warning(\n",
    "                    f\"No local mount to {train_mount} found. Therefore, switching to remote data synchronization mode via MinIO. This will work but is slower compared to local mounts. Consider adding a mount to '{train_mount}' for this component by using a PVC inside your pipeline.\"\n",
    "                )\n",
    "                is_remote = True\n",
    "        else:\n",
    "            # see: https://github.com/kubernetes-client/python/blob/6d4587e18064288d031ed9bbf5ab5b8245460b3c/examples/remote_cluster.py\n",
    "            logger.info(\n",
    "                \"Remote host and token found. Using remote cluster configuration...\"\n",
    "            )\n",
    "            configuration = client.Configuration()\n",
    "            configuration.host = cluster_configuration[\"remote-host\"]\n",
    "            configuration.verify_ssl = False\n",
    "            configuration.api_key = {\n",
    "                \"authorization\": \"Bearer \" + cluster_configuration[\"remote-token\"]\n",
    "            }\n",
    "            api_client = client.ApiClient(configuration)\n",
    "            is_remote = True\n",
    "\n",
    "        return (api_client, is_remote)\n",
    "\n",
    "    def clone_path(source, target):\n",
    "        try:\n",
    "            logger.info(f\"Cloning source path {source} to {target} of training job...\")\n",
    "            shutil.copytree(source, target)\n",
    "            logger.info(\"Cloning finished. Target path contents:\")\n",
    "            logger.info(os.listdir(target))\n",
    "        except OSError as e:\n",
    "            if e.errno in (errno.ENOTDIR, errno.EINVAL):\n",
    "                shutil.copy(source, target)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def sync_with_minio(\n",
    "        cluster_configuration: dict,\n",
    "        inputs: dict,\n",
    "        job_name: str,\n",
    "        is_upload: bool,\n",
    "        remove_minio_files: bool = False,\n",
    "    ):\n",
    "        import boto3\n",
    "        import botocore\n",
    "        from botocore.client import Config\n",
    "        import json\n",
    "        import logging\n",
    "        import os\n",
    "        import sys\n",
    "        import tarfile\n",
    "\n",
    "        logging.basicConfig(\n",
    "            stream=sys.stdout,\n",
    "            level=logging.INFO,\n",
    "            format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "        )\n",
    "        logger = logging.getLogger()\n",
    "\n",
    "        def establish_minio_connection(cluster_configuration):\n",
    "            if (\"minio-accesskey\" in cluster_configuration) and (\n",
    "                \"minio-secretkey\" in cluster_configuration\n",
    "            ):\n",
    "                minio_user = cluster_configuration[\"minio-accesskey\"]\n",
    "                minio_pass = cluster_configuration[\"minio-secretkey\"]\n",
    "            else:\n",
    "                minio_user = os.getenv(\"MINIO_USER\")\n",
    "                minio_pass = os.getenv(\"MINIO_PASS\")\n",
    "\n",
    "            if minio_user == \"\" or minio_pass == \"\":\n",
    "                err = \"Environment variables MINIO_USER and MINIO_PASS need externally to be provided to this component using k8s_secret_key_to_env!\"\n",
    "                raise Exception(err)\n",
    "\n",
    "            return boto3.session.Session().resource(\n",
    "                service_name=\"s3\",\n",
    "                endpoint_url=cluster_configuration[\"minio-url\"],\n",
    "                aws_access_key_id=minio_user,\n",
    "                aws_secret_access_key=minio_pass,\n",
    "                config=Config(signature_version=\"s3v4\"),\n",
    "            )\n",
    "\n",
    "        def path_to_tarfilename(pathname):\n",
    "            return f\"{pathname.replace(os.sep, '-')}.tar.gz\"\n",
    "\n",
    "        def make_tarfile(output_filename, source_dir):\n",
    "            with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "                tar.add(source_dir, arcname=\".\")\n",
    "\n",
    "        # see: https://stackoverflow.com/a/47565719/2625096\n",
    "        def bucket_exists(minio_client, bucket):\n",
    "            try:\n",
    "                minio_client.meta.client.head_bucket(Bucket=bucket.name)\n",
    "                return True\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                error_code = int(e.response[\"Error\"][\"Code\"])\n",
    "                if error_code == 403:\n",
    "                    # Forbidden Access -> Private Bucket\n",
    "                    return True\n",
    "                elif error_code == 404:\n",
    "                    return False\n",
    "\n",
    "        def upload_to_minio(file, upload_bucket, job_folder, job_name, minio_client):\n",
    "            bucket = minio_client.Bucket(upload_bucket)\n",
    "\n",
    "            if not bucket_exists(minio_client, bucket):\n",
    "                minio_client.create_bucket(Bucket=bucket.name)\n",
    "\n",
    "            bucket.upload_file(file, f\"{job_folder}/{job_name}/{file}\")\n",
    "\n",
    "        def download_from_minio(\n",
    "            file, upload_bucket, job_folder, job_name, minio_client, remove_minio_file\n",
    "        ):\n",
    "            bucket = minio_client.Bucket(upload_bucket)\n",
    "            key = f\"{job_folder}/{job_name}/{file}\"\n",
    "\n",
    "            bucket.download_file(key, file)\n",
    "\n",
    "            if remove_minio_file:\n",
    "                bucket.Object(key).delete()\n",
    "\n",
    "        def extract_tarfile(tarfile_name, target):\n",
    "            with tarfile.open(tarfile_name, \"r:gz\") as tar_gz_ref:\n",
    "                tar_gz_ref.extractall(target)\n",
    "\n",
    "        if isinstance(cluster_configuration, str):\n",
    "            cluster_configuration = json.loads(cluster_configuration)\n",
    "\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = json.loads(inputs)\n",
    "\n",
    "        if isinstance(is_upload, str):\n",
    "            if is_upload == \"True\":\n",
    "                is_upload = True\n",
    "            else:\n",
    "                is_upload = False\n",
    "\n",
    "        logger.info(\"Establishing MinIO connection...\")\n",
    "        minio_client = establish_minio_connection(cluster_configuration)\n",
    "\n",
    "        for (source, target) in inputs:\n",
    "            tarfilename = path_to_tarfilename(source)\n",
    "\n",
    "            if is_upload:\n",
    "                logger.info(f\"Tar.gz input {source} into {tarfilename}...\")\n",
    "                make_tarfile(tarfilename, source)\n",
    "\n",
    "                logger.info(\n",
    "                    f'Uploading {tarfilename} to {cluster_configuration[\"minio-bucket\"]}/{cluster_configuration[\"minio-job-folder\"]}/{job_name}/{tarfilename}...'\n",
    "                )\n",
    "                upload_to_minio(\n",
    "                    tarfilename,\n",
    "                    cluster_configuration[\"minio-bucket\"],\n",
    "                    cluster_configuration[\"minio-job-folder\"],\n",
    "                    job_name,\n",
    "                    minio_client,\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f'Downloading {cluster_configuration[\"minio-bucket\"]}/{cluster_configuration[\"minio-job-folder\"]}/{job_name}/{tarfilename} to {tarfilename}...'\n",
    "                )\n",
    "                download_from_minio(\n",
    "                    tarfilename,\n",
    "                    cluster_configuration[\"minio-bucket\"],\n",
    "                    cluster_configuration[\"minio-job-folder\"],\n",
    "                    job_name,\n",
    "                    minio_client,\n",
    "                    remove_minio_files,\n",
    "                )\n",
    "\n",
    "                logger.info(f\"Extracting {tarfilename} to {target}...\")\n",
    "                extract_tarfile(tarfilename, target)\n",
    "\n",
    "                logger.info(\"Result:\")\n",
    "                logger.info(os.listdir(target))\n",
    "\n",
    "    def generate_unique_job_name(model_name: str):\n",
    "        epoch = datetime.today().strftime(\"%Y%m%d%H%M%S\")\n",
    "        return f\"job-{model_name}-{epoch}\"\n",
    "\n",
    "    def get_current_namespace():\n",
    "        SA_NAMESPACE = \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n",
    "        with open(SA_NAMESPACE) as f:\n",
    "            return f.read()\n",
    "\n",
    "    def initialize_namespace(namespace: str):\n",
    "        if namespace == \"\":\n",
    "            namespace = get_current_namespace()\n",
    "        namespace_spec = f\"namespace: {namespace}\"\n",
    "\n",
    "        return (namespace, namespace_spec)\n",
    "\n",
    "    def initialize_nodeselector(node_selector: str):\n",
    "        if node_selector != \"\":\n",
    "            node_selector = f\"nodeSelector:\\n        {node_selector}\"\n",
    "        return node_selector\n",
    "\n",
    "    def initialize_init_container(\n",
    "        base_image: str,\n",
    "        cluster_configuration: Dict[str, str],\n",
    "        inputs: Dict[str, str],\n",
    "        is_remote: bool,\n",
    "        job_name: str,\n",
    "        minio_secret: str,\n",
    "        mount_path: str,\n",
    "    ):\n",
    "        if not is_remote:\n",
    "            return \"\"\n",
    "\n",
    "        command_specification = kfp.components.func_to_component_text(\n",
    "            func=sync_with_minio\n",
    "        )\n",
    "\n",
    "        # inner components loose type information as needed by lists/dicts\n",
    "        # -> cluster_configuration & inputs need to be a string (using json)\n",
    "        cluster_configuration_json = json.dumps(\n",
    "            {\n",
    "                \"minio-bucket\": cluster_configuration[\"minio-bucket\"],\n",
    "                \"minio-job-folder\": cluster_configuration[\"minio-job-folder\"],\n",
    "                \"minio-url\": cluster_configuration[\"minio-url\"],\n",
    "            }\n",
    "        )\n",
    "        inputs_json = json.dumps(inputs)\n",
    "        parameters = {\n",
    "            \"cluster_configuration\": cluster_configuration_json,\n",
    "            \"inputs\": inputs_json,\n",
    "            \"job_name\": job_name,\n",
    "            \"is_upload\": \"False\",\n",
    "        }\n",
    "\n",
    "        command, _, _ = initialize_command(command_specification, parameters)\n",
    "\n",
    "        init_container = f\"\"\"initContainers:\n",
    "          - name: init-inputs\n",
    "            image: {base_image}\n",
    "            command: {command}\n",
    "            volumeMounts:\n",
    "            - mountPath: {mount_path}\n",
    "              name: training\n",
    "            env:\n",
    "            - name: MINIO_USER\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: accesskey\n",
    "                  optional: false\n",
    "            - name: MINIO_PASS\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: secretkey\n",
    "                  optional: false\n",
    "\"\"\"\n",
    "        return init_container\n",
    "\n",
    "    def initialize_command(\n",
    "        specification: str,\n",
    "        parameters: Dict[str, str],\n",
    "        path_parameters: Dict[str, str] = {},\n",
    "        mount_path: str = \"/tmp\",\n",
    "    ):\n",
    "        component_yaml = yaml.safe_load(specification)\n",
    "        container_yaml = component_yaml[\"implementation\"][\"container\"]\n",
    "        command = container_yaml[\"command\"]\n",
    "        args = container_yaml[\"args\"]\n",
    "\n",
    "        actual_args = list()\n",
    "        inputs = list()\n",
    "        outputs = list()\n",
    "        for idx, arg in enumerate(args):\n",
    "            if type(arg) is dict:\n",
    "                if \"inputValue\" in arg:\n",
    "                    # required parameter (value)\n",
    "                    key = arg[\"inputValue\"]\n",
    "                    if key in parameters:\n",
    "                        actual_args.append(parameters[key])\n",
    "                    else:\n",
    "                        err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                        raise Exception(err)\n",
    "                elif \"if\" in arg:\n",
    "                    # optional parameter\n",
    "                    key = arg[\"if\"][\"cond\"][\"isPresent\"]\n",
    "                    if key in parameters:\n",
    "                        actual_args.append(f\"--{key}\")\n",
    "                        actual_args.append(parameters[key])\n",
    "                elif \"inputPath\" in arg:\n",
    "                    # required InputPath\n",
    "                    key = arg[\"inputPath\"]\n",
    "                    if key in parameters:\n",
    "                        path_key = parameters[key]\n",
    "                        if path_key in path_parameters:\n",
    "                            mount = f\"{mount_path}{path_parameters[path_key]}\"\n",
    "                            inputs.append((path_parameters[path_key], mount))\n",
    "                            actual_args.append(mount)\n",
    "                        else:\n",
    "                            err = f\"InputPath '{path_key}' unavailable in training component!\"\n",
    "                            raise Exception(err)\n",
    "                    else:\n",
    "                        err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                        raise Exception(err)\n",
    "                elif \"outputPath\" in arg:\n",
    "                    # required OutputPath\n",
    "                    key = arg[\"outputPath\"]\n",
    "                    if key in parameters:\n",
    "                        path_key = parameters[key]\n",
    "                        if path_key in path_parameters:\n",
    "                            mount = f\"{mount_path}{path_parameters[path_key]}\"\n",
    "                            outputs.append((mount, path_parameters[path_key]))\n",
    "                            actual_args.append(mount)\n",
    "                        else:\n",
    "                            err = f\"OutputPath '{path_key}' unavailable in training component!\"\n",
    "                            raise Exception(err)\n",
    "                    else:\n",
    "                        err = f\"Required parameter '{key}' missing in component input!\"\n",
    "                        raise Exception(err)\n",
    "            else:\n",
    "                # required parameter (key)\n",
    "                actual_args.append(arg)\n",
    "\n",
    "        command_with_initialized_args = json.dumps(command + actual_args)\n",
    "\n",
    "        return command_with_initialized_args, inputs, outputs\n",
    "\n",
    "    def initialize_fetch_command(\n",
    "        cluster_configuration,\n",
    "        job_name: str,\n",
    "        outputs: Dict[str, str],\n",
    "    ):\n",
    "        command_specification = kfp.components.func_to_component_text(\n",
    "            func=sync_with_minio\n",
    "        )\n",
    "\n",
    "        # inner components loose type information as needed by lists/dicts\n",
    "        # -> cluster_configuration & inputs need to be a string (using json)\n",
    "        cluster_configuration_json = json.dumps(\n",
    "            {\n",
    "                \"minio-bucket\": cluster_configuration[\"minio-bucket\"],\n",
    "                \"minio-job-folder\": cluster_configuration[\"minio-job-folder\"],\n",
    "                \"minio-url\": cluster_configuration[\"minio-url\"],\n",
    "            }\n",
    "        )\n",
    "        outputs_json = json.dumps(outputs)\n",
    "        parameters = {\n",
    "            \"cluster_configuration\": cluster_configuration_json,\n",
    "            \"inputs\": outputs_json,\n",
    "            \"job_name\": job_name,\n",
    "            \"is_upload\": \"True\",\n",
    "        }\n",
    "        command, _, _ = initialize_command(command_specification, parameters)\n",
    "        return command\n",
    "\n",
    "    def create_pvc_spec(pvc_name, namespace_spec, access_mode, pvc_size):\n",
    "        pvc_spec = f\"\"\"apiVersion: batch/v1\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: {pvc_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  accessModes:\n",
    "  - {access_mode}\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: {pvc_size}\n",
    "\"\"\"\n",
    "        return yaml.safe_load(pvc_spec)\n",
    "\n",
    "    def create_minio_secret_spec(cluster_configuration, minio_secret, namespace_spec):\n",
    "        minio_secret_spec = f\"\"\"apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: {minio_secret}\n",
    "  {namespace_spec}\n",
    "stringData:\n",
    "  accesskey: {cluster_configuration[\"minio-accesskey\"]}\n",
    "  secretkey: {cluster_configuration[\"minio-secretkey\"]}\n",
    "\"\"\"\n",
    "        return yaml.safe_load(minio_secret_spec)\n",
    "\n",
    "    def create_train_job_configuration(\n",
    "        job_name,\n",
    "        namespace_spec,\n",
    "        node_selector,\n",
    "        base_image,\n",
    "        train_command,\n",
    "        train_mount,\n",
    "        cpus,\n",
    "        memory,\n",
    "        gpus,\n",
    "        init_container,\n",
    "        pvc_name,\n",
    "        distribution_specification,\n",
    "        minio_url,\n",
    "        minio_secret,\n",
    "        tensorboard_s3_address,\n",
    "    ):\n",
    "        if cpus:\n",
    "            cpu_spec = f\"cpu: {cpus}\"\n",
    "        else:\n",
    "            cpu_spec = \"\"\n",
    "\n",
    "        if memory:\n",
    "            memory_spec = f\"memory: {memory}\"\n",
    "        else:\n",
    "            memory_spec = \"\"\n",
    "\n",
    "        if gpus:\n",
    "            gpu_spec = f\"nvidia.com/gpu: {gpus}\"\n",
    "        else:\n",
    "            gpu_spec = \"\"\n",
    "\n",
    "        if distribution_specification is None:\n",
    "            distribution_specification = dict()\n",
    "\n",
    "        if \"distribution_type\" not in distribution_specification:\n",
    "            distribution_specification[\"distribution_type\"] = \"Job\"\n",
    "\n",
    "        if gpus < 1:\n",
    "            slots_per_worker = 1\n",
    "        else:\n",
    "            slots_per_worker = gpus\n",
    "\n",
    "        if \"number_of_workers\" in distribution_specification:\n",
    "            number_of_workers = distribution_specification[\"number_of_workers\"]\n",
    "        else:\n",
    "            number_of_workers = 2\n",
    "\n",
    "        number_of_processes = number_of_workers * slots_per_worker\n",
    "\n",
    "        if \"launcher_cpus\" in distribution_specification:\n",
    "            launcher_cpu_spec = f\"cpu: {distribution_specification['launcher_cpus']}\"\n",
    "        else:\n",
    "            launcher_cpu_spec = \"\"\n",
    "\n",
    "        if \"launcher_memory\" in distribution_specification:\n",
    "            launcher_memory_spec = (\n",
    "                f\"memory: {distribution_specification['launcher_memory']}\"\n",
    "            )\n",
    "        else:\n",
    "            launcher_memory_spec = \"\"\n",
    "\n",
    "        if \"worker_cpus\" in distribution_specification:\n",
    "            worker_cpu_spec = f\"cpu: {distribution_specification['worker_cpus']}\"\n",
    "        else:\n",
    "            worker_cpu_spec = \"\"\n",
    "\n",
    "        if \"worker_memory\" in distribution_specification:\n",
    "            worker_memory_spec = (\n",
    "                f\"memory: {distribution_specification['worker_memory']}\"\n",
    "            )\n",
    "        else:\n",
    "            worker_memory_spec = \"\"\n",
    "\n",
    "        if distribution_specification[\"distribution_type\"] == \"Job\":\n",
    "            job_spec = f\"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  labels:\n",
    "    train-model-job: {job_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "    spec:\n",
    "      {node_selector}\n",
    "      containers:\n",
    "        - name: training-container\n",
    "          image: {base_image}\n",
    "          command: {train_command}\n",
    "          volumeMounts:\n",
    "            - mountPath: {train_mount}\n",
    "              name: training\n",
    "          restartPolicy: Never\n",
    "          env:\n",
    "            - name: S3_ENDPOINT\n",
    "              value: {minio_url}\n",
    "            - name: AWS_ACCESS_KEY_ID\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: accesskey\n",
    "                  optional: false\n",
    "            - name: AWS_SECRET_ACCESS_KEY\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: secretkey\n",
    "                  optional: false\n",
    "            - name: AWS_S3_SIGNATURE_VERSION\n",
    "              value: \"s3v4\"\n",
    "            - name: TENSORBOARD_S3_ADDRESS\n",
    "              value: {tensorboard_s3_address}\n",
    "          resources:\n",
    "            limits:\n",
    "              {cpu_spec}\n",
    "              {memory_spec}\n",
    "              {gpu_spec}\n",
    "      {init_container}\n",
    "      volumes:\n",
    "        - name: training\n",
    "          persistentVolumeClaim:\n",
    "            claimName: {pvc_name}\n",
    "      restartPolicy: Never\n",
    "\"\"\"\n",
    "            job_config = {\n",
    "                \"group\": \"batch\",\n",
    "                \"version\": \"v1\",\n",
    "                \"plural\": \"jobs\",\n",
    "                \"label\": \"job-name\",\n",
    "            }\n",
    "        elif distribution_specification[\"distribution_type\"] == \"MPI\":\n",
    "            job_spec = f\"\"\"apiVersion: kubeflow.org/v1\n",
    "kind: MPIJob\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  labels:\n",
    "    train-model-job: {job_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  slotsPerWorker: {slots_per_worker}\n",
    "  runPolicy:\n",
    "    cleanPodPolicy: Running\n",
    "  mpiReplicaSpecs:\n",
    "    Launcher:\n",
    "      replicas: 1\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          {init_container}\n",
    "          volumes:\n",
    "            - name: training\n",
    "              persistentVolumeClaim:\n",
    "                claimName: {pvc_name}\n",
    "          containers:\n",
    "          - image: {base_image}\n",
    "            name: mpi-launcher\n",
    "            command:\n",
    "            - mpirun\n",
    "            - -np\n",
    "            - \"{number_of_processes}\"\n",
    "            - --allow-run-as-root\n",
    "            - -bind-to\n",
    "            - none\n",
    "            - -map-by\n",
    "            - slot\n",
    "            - --prefix\n",
    "            - /opt/conda\n",
    "            - -mca\n",
    "            - pml\n",
    "            - ob1\n",
    "            - -mca\n",
    "            - btl\n",
    "            - ^openib\n",
    "            - -x\n",
    "            - NCCL_DEBUG=INFO\n",
    "            args: {train_command}\n",
    "            resources:\n",
    "              limits:\n",
    "                {launcher_cpu_spec}\n",
    "                {launcher_memory_spec}\n",
    "    Worker:\n",
    "      replicas: {number_of_workers}\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          {node_selector}\n",
    "          containers:\n",
    "          - image: {base_image}\n",
    "            name: mpi-worker\n",
    "            env:\n",
    "            - name: S3_ENDPOINT\n",
    "              value: {minio_url}\n",
    "            - name: AWS_ACCESS_KEY_ID\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: accesskey\n",
    "                  optional: false\n",
    "            - name: AWS_SECRET_ACCESS_KEY\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {minio_secret}\n",
    "                  key: secretkey\n",
    "                  optional: false\n",
    "            - name: AWS_S3_SIGNATURE_VERSION\n",
    "              value: \"s3v4\"\n",
    "            - name: TENSORBOARD_S3_ADDRESS\n",
    "              value: {tensorboard_s3_address}\n",
    "            volumeMounts:\n",
    "              - mountPath: /train\n",
    "                name: training\n",
    "            resources:\n",
    "              limits:\n",
    "                {worker_cpu_spec}\n",
    "                {worker_memory_spec}\n",
    "                {gpu_spec}\n",
    "          volumes:\n",
    "            - name: training\n",
    "              persistentVolumeClaim:\n",
    "                claimName: {pvc_name}\n",
    "\"\"\"\n",
    "            job_config = {\n",
    "                \"group\": \"kubeflow.org\",\n",
    "                \"version\": \"v1\",\n",
    "                \"plural\": \"mpijobs\",\n",
    "                \"label\": \"training.kubeflow.org/replica-type=launcher,training.kubeflow.org/job-name\",\n",
    "            }\n",
    "        elif distribution_specification[\"distribution_type\"] == \"TF\":\n",
    "            job_spec = f\"\"\"apiVersion: kubeflow.org/v1\n",
    "kind: TFJob\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  labels:\n",
    "    train-model-job: {job_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  runPolicy:\n",
    "    cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: {number_of_workers}\n",
    "      restartPolicy: OnFailure\n",
    "      template:\n",
    "        metadata:\n",
    "          annotations:\n",
    "            sidecar.istio.io/inject: \"false\"\n",
    "        spec:\n",
    "          {node_selector}\n",
    "          containers:\n",
    "            - name: tensorflow\n",
    "              image: {base_image}\n",
    "              command: {train_command}\n",
    "              env:\n",
    "              - name: S3_ENDPOINT\n",
    "                value: {minio_url}\n",
    "              - name: AWS_ACCESS_KEY_ID\n",
    "                valueFrom:\n",
    "                  secretKeyRef:\n",
    "                    name: {minio_secret}\n",
    "                    key: accesskey\n",
    "                    optional: false\n",
    "              - name: AWS_SECRET_ACCESS_KEY\n",
    "                valueFrom:\n",
    "                  secretKeyRef:\n",
    "                    name: {minio_secret}\n",
    "                    key: secretkey\n",
    "                    optional: false\n",
    "              - name: AWS_S3_SIGNATURE_VERSION\n",
    "                value: \"s3v4\"\n",
    "              - name: TENSORBOARD_S3_ADDRESS\n",
    "                value: {tensorboard_s3_address}\n",
    "              volumeMounts:\n",
    "                - mountPath: /train\n",
    "                  name: training\n",
    "              resources:\n",
    "                limits:\n",
    "                  {worker_cpu_spec}\n",
    "                  {worker_memory_spec}\n",
    "                  {gpu_spec}\n",
    "          volumes:\n",
    "            - name: training\n",
    "              persistentVolumeClaim:\n",
    "                claimName: {pvc_name}\n",
    "\"\"\"\n",
    "            job_config = {\n",
    "                \"group\": \"kubeflow.org\",\n",
    "                \"version\": \"v1\",\n",
    "                \"plural\": \"tfjobs\",\n",
    "                \"label\": \"tf-job-name\",\n",
    "            }\n",
    "        else:\n",
    "            err = f\"Job failed while executing - unknown distribution_type: {distribution_specification['distribution_type']}\"\n",
    "            raise Exception(err)\n",
    "\n",
    "        job_config[\"job_spec\"] = yaml.safe_load(job_spec)\n",
    "        return job_config\n",
    "\n",
    "    def create_fetch_job_configuration(\n",
    "        job_name,\n",
    "        namespace_spec,\n",
    "        base_image,\n",
    "        fetch_command,\n",
    "        train_mount,\n",
    "        minio_secret,\n",
    "        pvc_name,\n",
    "    ):\n",
    "        job_spec = f\"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  labels:\n",
    "    train-model-job: {job_name}\n",
    "  {namespace_spec}\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      annotations:\n",
    "        sidecar.istio.io/inject: \"false\"\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: training-container\n",
    "          image: {base_image}\n",
    "          command: {fetch_command}\n",
    "          volumeMounts:\n",
    "            - mountPath: {train_mount}\n",
    "              name: training\n",
    "          restartPolicy: Never\n",
    "          env:\n",
    "          - name: MINIO_USER\n",
    "            valueFrom:\n",
    "              secretKeyRef:\n",
    "                name: {minio_secret}\n",
    "                key: accesskey\n",
    "                optional: false\n",
    "          - name: MINIO_PASS\n",
    "            valueFrom:\n",
    "              secretKeyRef:\n",
    "                name: {minio_secret}\n",
    "                key: secretkey\n",
    "                optional: false\n",
    "      volumes:\n",
    "        - name: training\n",
    "          persistentVolumeClaim:\n",
    "            claimName: {pvc_name}\n",
    "      restartPolicy: Never\n",
    "\"\"\"\n",
    "        job_config = {\n",
    "            \"group\": \"batch\",\n",
    "            \"version\": \"v1\",\n",
    "            \"plural\": \"jobs\",\n",
    "            \"job_spec\": yaml.safe_load(job_spec),\n",
    "            \"label\": \"job-name\",\n",
    "        }\n",
    "        return job_config\n",
    "\n",
    "    def submit_and_monitor_job(\n",
    "        api_client, job_config, namespace, additional_job_resources=[]\n",
    "    ):\n",
    "        job_spec = job_config[\"job_spec\"]\n",
    "        job_resource = custom_object_api.create_namespaced_custom_object(\n",
    "            group=job_config[\"group\"],\n",
    "            version=job_config[\"version\"],\n",
    "            namespace=namespace,\n",
    "            plural=job_config[\"plural\"],\n",
    "            body=job_spec,\n",
    "        )\n",
    "        job_name = job_resource[\"metadata\"][\"name\"]\n",
    "        job_uid = job_resource[\"metadata\"][\"uid\"]\n",
    "\n",
    "        logger.info(\"Creating additional job resource...\")\n",
    "        if additional_job_resources:\n",
    "            for resource in additional_job_resources:\n",
    "                resource[\"metadata\"][\"ownerReferences\"] = [\n",
    "                    {\n",
    "                        \"apiVersion\": job_spec[\"apiVersion\"],\n",
    "                        \"kind\": job_spec[\"kind\"],\n",
    "                        \"name\": job_name,\n",
    "                        \"uid\": job_uid,\n",
    "                    }\n",
    "                ]\n",
    "            utils.create_from_yaml(api_client, yaml_objects=additional_job_resources)\n",
    "\n",
    "        logger.info(\"Waiting for job to succeed...\")\n",
    "        job_is_monitored = False\n",
    "        pods_being_monitored = set()\n",
    "        job_watch = watch.Watch()\n",
    "        for job_event in job_watch.stream(\n",
    "            custom_object_api.list_namespaced_custom_object,\n",
    "            group=job_config[\"group\"],\n",
    "            version=job_config[\"version\"],\n",
    "            plural=job_config[\"plural\"],\n",
    "            namespace=namespace,\n",
    "            label_selector=f\"train-model-job={job_name}\",\n",
    "            timeout_seconds=0,\n",
    "        ):\n",
    "            logger.info(f\"job_event: {job_event}\")\n",
    "            job = job_event[\"object\"]\n",
    "            if \"status\" not in job and \"items\" in job:\n",
    "                job = job[\"items\"][0]\n",
    "\n",
    "            if \"status\" not in job:\n",
    "                logger.info(\"Skipping event (no status information found)...\")\n",
    "                continue\n",
    "\n",
    "            job_status = dict()\n",
    "            if \"active\" in job[\"status\"]:\n",
    "                job_status[\"active\"] = job[\"status\"][\"active\"]\n",
    "            else:\n",
    "                job_status[\"active\"] = 0\n",
    "            if \"completionTime\" in job[\"status\"]:\n",
    "                job_status[\"completionTime\"] = job[\"status\"][\"completionTime\"]\n",
    "            if \"failed\" in job[\"status\"]:\n",
    "                job_status[\"failed\"] = job[\"status\"][\"failed\"]\n",
    "            else:\n",
    "                job_status[\"failed\"] = 0\n",
    "            if \"ready\" in job[\"status\"]:\n",
    "                job_status[\"ready\"] = job[\"status\"][\"ready\"]\n",
    "            else:\n",
    "                job_status[\"ready\"] = 0\n",
    "            if \"startTime\" in job[\"status\"]:\n",
    "                job_status[\"startTime\"] = job[\"status\"][\"startTime\"]\n",
    "            if \"succeeded\" in job[\"status\"]:\n",
    "                job_status[\"succeeded\"] = job[\"status\"][\"succeeded\"]\n",
    "            else:\n",
    "                job_status[\"succeeded\"] = 0\n",
    "\n",
    "            # MPI\n",
    "            job_status[\"Complete\"] = \"False\"\n",
    "            job_status[\"Created\"] = \"False\"\n",
    "            job_status[\"Failed\"] = \"False\"\n",
    "            job_status[\"Running\"] = \"False\"\n",
    "            job_status[\"Succeeded\"] = \"False\"\n",
    "            if \"conditions\" in job[\"status\"]:\n",
    "                for condition in job[\"status\"][\"conditions\"]:\n",
    "                    job_status[condition[\"type\"]] = condition[\"status\"]\n",
    "\n",
    "            logger.info(f\"Job status: {job_status}\")\n",
    "\n",
    "            def start_monitoring(job_name, job_status):\n",
    "                return (not job_is_monitored) and (\n",
    "                    job_status[\"active\"] > 0\n",
    "                    or job_status[\"Running\"] == \"True\"\n",
    "                    or job_status[\"failed\"] > 0\n",
    "                    or job_status[\"Failed\"] == \"True\"\n",
    "                    or job_status[\"ready\"] > 0\n",
    "                    or job_status[\"Complete\"] == \"True\"\n",
    "                    or job_status[\"Succeeded\"] == \"True\"\n",
    "                )\n",
    "\n",
    "            if start_monitoring(job_name, job_status):\n",
    "                job_is_monitored = True\n",
    "                logger.info(\"Monitoring pods of job...\")\n",
    "\n",
    "                # See https://stackoverflow.com/questions/65938572/kubernetes-python-client-equivalent-of-kubectl-wait-for-command\n",
    "                pod_watch = watch.Watch()\n",
    "                for pod_event in pod_watch.stream(\n",
    "                    func=core_api.list_namespaced_pod,\n",
    "                    namespace=namespace,\n",
    "                    label_selector=f\"{job_config['label']}={job_name}\",\n",
    "                    timeout_seconds=0,\n",
    "                ):\n",
    "                    pod = pod_event[\"object\"]\n",
    "                    pod_name = pod.metadata.name\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"Pod {pod_name}: {pod_event['type']} - {pod.status.phase}\"\n",
    "                    )\n",
    "\n",
    "                    if pod_name in pods_being_monitored:\n",
    "                        pod_watch.stop()\n",
    "                    elif pod_name not in pods_being_monitored and (\n",
    "                        pod.status.phase == \"Running\"\n",
    "                        or pod.status.phase == \"Succeeded\"\n",
    "                        or pod.status.phase == \"Failed\"\n",
    "                    ):\n",
    "                        pods_being_monitored.add(pod_name)\n",
    "                        logger.info(\n",
    "                            \"==============================================================================\"\n",
    "                        )\n",
    "                        logger.info(\n",
    "                            \"==============================================================================\"\n",
    "                        )\n",
    "                        logger.info(f\"=== Streaming logs of pod {pod_name}...\")\n",
    "                        logger.info(\n",
    "                            \"==============================================================================\"\n",
    "                        )\n",
    "                        logger.info(\n",
    "                            \"==============================================================================\"\n",
    "                        )\n",
    "\n",
    "                        log_watch = watch.Watch()\n",
    "                        for log_event in log_watch.stream(\n",
    "                            core_api.read_namespaced_pod_log,\n",
    "                            name=pod_name,\n",
    "                            namespace=namespace,\n",
    "                            follow=True,\n",
    "                            _return_http_data_only=True,\n",
    "                            _preload_content=False,\n",
    "                        ):\n",
    "                            print(log_event)\n",
    "                        logger.info(\n",
    "                            \"==============================================================================\"\n",
    "                        )\n",
    "                        logger.info(\n",
    "                            \"==============================================================================\"\n",
    "                        )\n",
    "\n",
    "                        pod_watch.stop()\n",
    "\n",
    "                        if pod.status.phase == \"Failed\":\n",
    "                            err = \"Job failed while executing.\"\n",
    "                            raise Exception(err)\n",
    "                        break\n",
    "                    if pod_event[\"type\"] == \"DELETED\":\n",
    "                        err = \"Pod was deleted while we where waiting for it to start.\"\n",
    "                        raise Exception(err)\n",
    "            elif (\n",
    "                job_status[\"succeeded\"] > 0\n",
    "                or job_status[\"Complete\"] == \"True\"\n",
    "                or job_status[\"Succeeded\"] == \"True\"\n",
    "            ):\n",
    "                job_watch.stop()\n",
    "                logger.info(\"Job finished successfully.\")\n",
    "                break\n",
    "            elif not (job_status[\"active\"] > 0 or job_status[\"Running\"] == \"True\") and (\n",
    "                job_status[\"failed\"] > 0 or job_status[\"Failed\"] == \"True\"\n",
    "            ):\n",
    "                job_watch.stop()\n",
    "                raise Exception(\"Job failed!\")\n",
    "            else:\n",
    "                logger.info(f\"Waiting for job updates. Current status: {job_status}\")\n",
    "\n",
    "    ###########################################################################\n",
    "    # Main Workflow\n",
    "    ###########################################################################\n",
    "\n",
    "    logger.info(\"Establishing local cluster connection...\")\n",
    "    local_api_client = establish_local_cluster_connection()\n",
    "\n",
    "    logger.info(\"Receiving training cluster configuration...\")\n",
    "    cluster_configuration = get_cluster_configuration(\n",
    "        local_api_client, cluster_configuration_secret\n",
    "    )\n",
    "\n",
    "    logger.info(\"Establishing training cluster connection...\")\n",
    "    api_client, is_remote = establish_training_cluster_connection(\n",
    "        local_api_client, cluster_configuration\n",
    "    )\n",
    "    batch_api = client.BatchV1Api(api_client)\n",
    "    core_api = client.CoreV1Api(api_client)\n",
    "    custom_object_api = client.CustomObjectsApi(api_client)\n",
    "\n",
    "    logger.info(\"Initializing resources...\")\n",
    "    job_name = generate_unique_job_name(model_name)\n",
    "    job_minio_secret = f\"{job_name}-minio-secret\"\n",
    "    namespace, namespace_spec = initialize_namespace(\n",
    "        cluster_configuration[\"remote-namespace\"]\n",
    "    )\n",
    "    pvc_name = f\"{job_name}-pvc\"\n",
    "    node_selector = initialize_nodeselector(node_selector)\n",
    "\n",
    "    path_parameters = {\n",
    "        \"train_dataset_dir\": train_dataset_dir,\n",
    "        \"validation_dataset_dir\": validation_dataset_dir,\n",
    "        \"model_dir\": model_dir,\n",
    "    }\n",
    "    train_command, inputs, outputs = initialize_command(\n",
    "        train_specification, train_parameters, path_parameters, train_mount\n",
    "    )\n",
    "\n",
    "    init_container = initialize_init_container(\n",
    "        base_image,\n",
    "        cluster_configuration,\n",
    "        inputs,\n",
    "        is_remote,\n",
    "        job_name,\n",
    "        job_minio_secret,\n",
    "        train_mount,\n",
    "    )\n",
    "\n",
    "    logger.info(\"=======================================\")\n",
    "    logger.info(\"Derived configurations\")\n",
    "    logger.info(\"=======================================\")\n",
    "    logger.info(f\"job_name: {job_name}\")\n",
    "    logger.info(f\"namespace: {namespace}\")\n",
    "    logger.info(f\"is_remote: {is_remote}\")\n",
    "    logger.info(f\"minio_url: {cluster_configuration['minio-url']}\")\n",
    "    logger.info(f\"job_minio_secret: {job_minio_secret}\")\n",
    "    logger.info(\"inputs (input paths send to job):\")\n",
    "    for source, target in inputs:\n",
    "        logger.info(\n",
    "            f\"- {source} -> {cluster_configuration['minio-bucket']}/{cluster_configuration['minio-job-folder']}/{job_name}/{target}\"\n",
    "        )\n",
    "    logger.info(\"outputs (output paths returning from job):\")\n",
    "    for source, target in outputs:\n",
    "        logger.info(\n",
    "            f\"- {target} <- {cluster_configuration['minio-bucket']}/{cluster_configuration['minio-job-folder']}/{job_name}/{source}\"\n",
    "        )\n",
    "    logger.info(f\"distribution_specification: {distribution_specification}\")\n",
    "    logger.info(f\"train_command: {train_command}\")\n",
    "    logger.info(\"=======================================\")\n",
    "\n",
    "    additional_job_resources = []\n",
    "\n",
    "    if is_remote:\n",
    "        logger.info(\"Using MinIO to sync data with a new remote PVC for the job...\")\n",
    "        sync_with_minio(cluster_configuration, inputs, job_name, is_upload=True)\n",
    "        additional_job_resources.append(\n",
    "            create_pvc_spec(\n",
    "                pvc_name, namespace_spec, cluster_configuration[\"access-mode\"], pvc_size\n",
    "            )\n",
    "        )\n",
    "        additional_job_resources.append(\n",
    "            create_minio_secret_spec(\n",
    "                cluster_configuration, job_minio_secret, namespace_spec\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Pushing inputs to local {train_mount} mount as shared with job environment...\"\n",
    "        )\n",
    "        for (source, target) in inputs:\n",
    "            clone_path(source, target)\n",
    "\n",
    "    logger.info(\"Creating train job configuration...\")\n",
    "    train_job_config = create_train_job_configuration(\n",
    "        job_name,\n",
    "        namespace_spec,\n",
    "        node_selector,\n",
    "        base_image,\n",
    "        train_command,\n",
    "        train_mount,\n",
    "        cpus,\n",
    "        memory,\n",
    "        gpus,\n",
    "        init_container,\n",
    "        pvc_name,\n",
    "        distribution_specification,\n",
    "        cluster_configuration[\"minio-url\"],\n",
    "        job_minio_secret,\n",
    "        tensorboard_s3_address,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Starting train job '{namespace}.{job_name}'...\")\n",
    "    submit_and_monitor_job(\n",
    "        api_client,\n",
    "        train_job_config,\n",
    "        namespace,\n",
    "        additional_job_resources,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Receiving training outputs...\")\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    if is_remote:\n",
    "        fetch_command = initialize_fetch_command(\n",
    "            cluster_configuration, job_name, outputs\n",
    "        )\n",
    "        fetch_job_name = f\"{job_name}-fetch\"\n",
    "\n",
    "        logger.info(\"Creating fetch job configuration...\")\n",
    "        fetch_job_config = create_fetch_job_configuration(\n",
    "            fetch_job_name,\n",
    "            namespace_spec,\n",
    "            base_image,\n",
    "            fetch_command,\n",
    "            train_mount,\n",
    "            job_minio_secret,\n",
    "            pvc_name,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting fetch job '{namespace}.{fetch_job_name}'...\")\n",
    "        submit_and_monitor_job(api_client, fetch_job_config, namespace)\n",
    "\n",
    "        logger.info(\"Fetching output data from MinIO & deleting it afterwards...\")\n",
    "        sync_with_minio(\n",
    "            cluster_configuration,\n",
    "            outputs,\n",
    "            job_name,\n",
    "            is_upload=False,\n",
    "            remove_minio_files=True,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Deleting Job {fetch_job_name}...\")\n",
    "        batch_api.delete_namespaced_job(fetch_job_name, namespace)\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Fetching outputs to local {train_mount} mount as shared with job environment...\"\n",
    "        )\n",
    "        for (source, target) in outputs:\n",
    "            clone_path(source, target)\n",
    "\n",
    "    logger.info(f\"Deleting Job {job_name}...\")\n",
    "    custom_object_api.delete_namespaced_custom_object(\n",
    "        train_job_config[\"group\"],\n",
    "        train_job_config[\"version\"],\n",
    "        namespace,\n",
    "        train_job_config[\"plural\"],\n",
    "        job_name,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Finished.\")\n",
    "\n",
    "\n",
    "train_model_job_comp = create_component_from_func(\n",
    "    func=Train_Model_Job, output_component_file=\"component.yaml\", base_image=BASE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cb66a-9c1f-4efe-82df-ca7b0dfe50c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
