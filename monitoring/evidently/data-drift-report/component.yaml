name: Data drift report
inputs:
- {name: dataset_dir, type: String}
- {name: ref_dataset_dir, type: String}
- {name: dataset_type, default: df, optional: true}
- {name: additional_args, type: JsonObject, optional: true}
- {name: column_mapping, type: JsonObject, optional: true}
outputs:
- {name: output_dir, type: String}
- {name: mlpipeline_ui_metadata}
implementation:
  container:
    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def data_drift_report(
          dataset_dir,
          ref_dataset_dir,
          output_dir,
          mlpipeline_ui_metadata_path,
          dataset_type="df",
          additional_args = None,
          column_mapping = None,
      ):

          from datasets import load_from_disk
          from evidently.metric_preset import DataDriftPreset
          from evidently.report import Report
          import json
          import logging
          import pandas as pd
          from pathlib import Path
          import sys

          logging.basicConfig(
              stream=sys.stdout,
              level=logging.INFO,
              format="%(levelname)s %(asctime)s: %(message)s",
          )

          def _process_dataframe(dataset, opt_args=None):
              return pd.read_pickle(dataset, **opt_args)

          def _process_dataframe_feather(dataset, opt_args=None):
              return pd.read_feather(dataset, **opt_args)

          def _process_csv(dataset, opt_args=None):
              return pd.read_csv(dataset, **opt_args)

          def _process_huggingface(dataset, opt_args=None):
              return load_from_disk(dataset).to_pandas()

          DATA_TYPES = {
              "df": _process_dataframe,
              "df/feather": _process_dataframe_feather,
              "csv": _process_csv,
              "huggingface": _process_huggingface,
          }

          def process_dataset(dataset, args=None):
              _dataset_type = dataset_type.lower()

              if dataset is None:
                  return None
              if _dataset_type not in DATA_TYPES.keys():
                  raise KeyError(
                      f"Dataset type {_dataset_type} not supported by the data quality component"
                  )
              return DATA_TYPES[_dataset_type](dataset, (args or {}))

          logging.info("Preparing datasets for data quality report...")
          df = process_dataset(dataset_dir, args=additional_args)
          ref_data = process_dataset(ref_dataset_dir, args=additional_args)

          report = Report(metrics=[DataDriftPreset()])

          logging.info("Generating report using Evidently...")
          report.run(current_data=df, reference_data=ref_data, column_mapping=column_mapping)

          logging.info("Saving report as HTML...")
          Path(output_dir).parent.mkdir(parents=True, exist_ok=True)
          report.save_html(output_dir)

          logging.info("Writing HTML content to Metadata UI...")
          html_content = open(output_dir, "r").read()
          metadata = {
              "outputs": [
                  {
                      "type": "web-app",
                      "storage": "inline",
                      "source": html_content,
                  }
              ]
          }

          with open(mlpipeline_ui_metadata_path, "w") as f:
              json.dump(metadata, f)

          logging.info("Finished.")

      import json
      import argparse
      _parser = argparse.ArgumentParser(prog='Data drift report', description='')
      _parser.add_argument("--dataset-dir", dest="dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--ref-dataset-dir", dest="ref_dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--dataset-type", dest="dataset_type", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--additional-args", dest="additional_args", type=json.loads, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--column-mapping", dest="column_mapping", type=json.loads, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--output-dir", dest="output_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())

      _outputs = data_drift_report(**_parsed_args)
    args:
    - --dataset-dir
    - {inputPath: dataset_dir}
    - --ref-dataset-dir
    - {inputPath: ref_dataset_dir}
    - if:
        cond: {isPresent: dataset_type}
        then:
        - --dataset-type
        - {inputValue: dataset_type}
    - if:
        cond: {isPresent: additional_args}
        then:
        - --additional-args
        - {inputValue: additional_args}
    - if:
        cond: {isPresent: column_mapping}
        then:
        - --column-mapping
        - {inputValue: column_mapping}
    - --output-dir
    - {outputPath: output_dir}
    - --mlpipeline-ui-metadata
    - {outputPath: mlpipeline_ui_metadata}
