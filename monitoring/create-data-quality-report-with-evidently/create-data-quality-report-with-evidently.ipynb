{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "from kfp.components import create_component_from_func, InputPath, OutputPath\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "\n",
    "\n",
    "def Create_Data_Quality_Report_with_Evidently(\n",
    "    dataset_dir: InputPath(str),\n",
    "    output_dir: OutputPath(str),\n",
    "    mlpipeline_ui_metadata_path: OutputPath(),\n",
    "    dataset_type=\"df\",\n",
    "    ref_dataset_dir: InputPath(str) = None,\n",
    "    additional_args: dict = None,\n",
    "    column_mapping: dict = None,\n",
    "    max_rows: int = 10000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a data quality report for a given dataset using Evidently AI.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): Path to the directory containing the dataset.\n",
    "        output_dir (str): Path to the directory where the report HTML file will be saved.\n",
    "        mlpipeline_ui_metadata_path (str): Path to the file where the metadata for the ML Pipeline UI will be saved.\n",
    "        dataset_type (str, optional): Type of the dataset. Must be one of 'csv', 'df' (dataframe via pickle), 'df/feather' (dataframe via feather), or 'huggingface'.\n",
    "            Defaults to 'df'.\n",
    "        ref_dataset_dir (str, optional): Path to the directory containing a reference dataset for comparison.\n",
    "            Defaults to None.\n",
    "        additional_args (dict, optional): Additional arguments to be passed to the dataset processing function.\n",
    "            Defaults to None.\n",
    "        column_mapping (dict, optional): Mapping of columns between the current and reference datasets.\n",
    "            Defaults to None.\n",
    "        max_rows (int, optional): Maximum number of rows to be used for producing the report. If dataset is larger, the report only selects the first max_rows rows of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        None: The function saves the report HTML file and metadata to disk.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the `dataset_type` argument is not one of the supported datatypes stated in the dataset_type parameter.\n",
    "    \"\"\"\n",
    "    from datasets import Array2D, Image, load_from_disk, Value\n",
    "    from evidently.metric_preset import DataQualityPreset\n",
    "    from evidently.report import Report\n",
    "    import hashlib\n",
    "    import json\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "\n",
    "    def _process_dataframe(dataset_dir, opt_args=None):\n",
    "        return pd.read_pickle(dataset_dir, **opt_args)\n",
    "\n",
    "    def _process_dataframe_feather(dataset_dir, opt_args=None):\n",
    "        return pd.read_feather(dataset_dir, **opt_args)\n",
    "\n",
    "    def _process_csv(dataset_dir, opt_args=None):\n",
    "        return pd.read_csv(dataset_dir, **opt_args)\n",
    "\n",
    "    def _process_huggingface(dataset_dir, opt_args=None):\n",
    "        dataset = load_from_disk(dataset_dir)\n",
    "\n",
    "        if (opt_args is not None) and (\"split\" in opt_args):\n",
    "            logging.info(f\"Selecting split '{opt_args['split']}'...\")\n",
    "            dataset = dataset[opt_args[\"split\"]]\n",
    "\n",
    "        def to_hash(encoded_text):\n",
    "            hash_object = hashlib.md5(encoded_text)\n",
    "            return hash_object.hexdigest()\n",
    "\n",
    "        def list_to_str(examples):\n",
    "            for key in arrays:\n",
    "                examples[key] = [\n",
    "                    to_hash(\"\".join(str(value) for value in a_list).encode())\n",
    "                    for a_list in examples[key]\n",
    "                ]\n",
    "\n",
    "            for key in images:\n",
    "                examples[key] = [\n",
    "                    to_hash(an_image.tobytes()) for an_image in examples[key]\n",
    "                ]\n",
    "            return examples\n",
    "\n",
    "        arrays = set()\n",
    "        images = set()\n",
    "        for key, feature in dataset.features.items():\n",
    "            if isinstance(feature, Array2D):\n",
    "                arrays.add(key)\n",
    "            if isinstance(feature, Image):\n",
    "                images.add(key)\n",
    "\n",
    "        copy_of_features = dataset.features.copy()\n",
    "        for feature in arrays:\n",
    "            copy_of_features[feature] = Value(dtype=\"string\", id=None)\n",
    "        for feature in images:\n",
    "            copy_of_features[feature] = Value(dtype=\"string\", id=None)\n",
    "\n",
    "        string_dataset = dataset.map(\n",
    "            list_to_str,\n",
    "            batched=True,\n",
    "            batch_size=100,\n",
    "            num_proc=1,\n",
    "            keep_in_memory=True,\n",
    "            features=copy_of_features,\n",
    "        )\n",
    "        return string_dataset.to_pandas()\n",
    "\n",
    "    DATA_TYPES = {\n",
    "        \"csv\": _process_csv,\n",
    "        \"df\": _process_dataframe,\n",
    "        \"df/feather\": _process_dataframe_feather,\n",
    "        \"huggingface\": _process_huggingface,\n",
    "    }\n",
    "\n",
    "    def process_dataset(dataset_dir, args=None):\n",
    "        _dataset_type = dataset_type.lower()\n",
    "\n",
    "        if dataset_dir is None:\n",
    "            return None\n",
    "        if _dataset_type not in DATA_TYPES.keys():\n",
    "            raise KeyError(\n",
    "                f\"Dataset type {_dataset_type} not supported by the data quality component\"\n",
    "            )\n",
    "\n",
    "        dataset = DATA_TYPES[_dataset_type](dataset_dir, (args or {}))\n",
    "\n",
    "        if max_rows is not None:\n",
    "            dataset = dataset.head(max_rows)\n",
    "        return dataset\n",
    "\n",
    "    logging.info(\"Preparing datasets for data quality report...\")\n",
    "    df = process_dataset(dataset_dir, args=additional_args)\n",
    "    ref_data = process_dataset(ref_dataset_dir, args=additional_args)\n",
    "\n",
    "    report = Report(metrics=[DataQualityPreset()])\n",
    "\n",
    "    logging.info(\"Generating report using Evidently...\")\n",
    "    report.run(current_data=df, reference_data=ref_data, column_mapping=column_mapping)\n",
    "\n",
    "    logging.info(\"Saving report as HTML...\")\n",
    "    Path(output_dir).parent.mkdir(parents=True, exist_ok=True)\n",
    "    report.save_html(output_dir)\n",
    "\n",
    "    logging.info(\"Writing HTML content to Metadata UI...\")\n",
    "    html_content = open(output_dir, \"r\").read()\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"web-app\",\n",
    "                \"storage\": \"inline\",\n",
    "                \"source\": html_content,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(mlpipeline_ui_metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    logging.info(\"Finished.\")\n",
    "\n",
    "\n",
    "data_quality_report_op = create_component_from_func(\n",
    "    Create_Data_Quality_Report_with_Evidently,\n",
    "    output_component_file=\"component.yaml\",\n",
    "    base_image=BASE_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfa3bc2375f864739603e288c05d4bfd658f5dbc82f0120480866d89037421e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
