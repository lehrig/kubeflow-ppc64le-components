name: Deploy model with kserve
description: Deploys a model using KServe and Trino as backend.
inputs:
- {name: project_name, type: String, description: 'Name of the project. Must be unique
    for the targeted namespace and conform Kubernetes naming conventions. Example:
    my-model.'}
- {name: model_version, type: Integer, description: 'Version of the deployed model.
    Relevant to match explainer version to model version. Example: 1.'}
- {name: explainer_type, type: String, description: 'Type of Alibi explanation. If
    None, explanations are not provided. Example: AnchorTabular.', optional: true}
- {name: kserve_version, type: String, description: 'KServe API version. Example:
    v1beta1.', default: v1beta1, optional: true}
- {name: s3_bucket, type: String, description: 'Name of the s3 bucket in which model
    projects reside. Example: projects.', default: projects, optional: true}
outputs:
- {name: Output, type: String}
implementation:
  container:
    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'kserve' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'kserve' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def deploy_model_with_kserve(
          project_name,
          model_version,
          explainer_type = None,
          kserve_version = "v1beta1",
          s3_bucket = "projects",
      ):
          """
          Deploys a model using KServe and Trino as backend.

                  Parameters:
                          project_name: Name of the project. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.
                          explainer_type: Type of Alibi explanation. If None, explanations are not provided. Example: AnchorTabular.
                          kserve_version: KServe API version. Example: v1beta1.
                          model_version: Version of the deployed model. Relevant to match explainer version to model version. Example: 1.
                          s3_bucket: Name of the s3 bucket in which model projects reside. Example: projects.
                  Returns:
                          endpoint: REST endpoint where the model can be queried. Example: https://my-model-user-example-com.apps.myorg.com.
          """
          from kubernetes import client, config
          from kserve import KServeClient
          from kserve import constants
          from kserve import utils
          from kserve import V1beta1AlibiExplainerSpec
          from kserve import V1beta1ExplainerSpec
          from kserve import V1beta1InferenceService
          from kserve import V1beta1InferenceServiceSpec
          from kserve import V1beta1PredictorSpec
          from kserve import V1beta1TritonSpec
          import logging
          import sys

          logging.basicConfig(
              stream=sys.stdout,
              level=logging.INFO,
              format="%(levelname)s %(asctime)s: %(message)s",
          )

          try:
              model_version = int(model_version)
          except ValueError as e:
              logging.warning(
                  "Could not parse model version. Continuing with default value 1..."
              )
              model_version = 1

          # See: https://www.kubeflow.org/docs/external-add-ons/kserve/first_isvc_kserve/
          logging.info("Initializing environment...")
          config.load_incluster_config()
          namespace = utils.get_default_target_namespace()
          api_version = constants.KSERVE_GROUP + "/" + kserve_version
          storage_uri: str = f"s3://{s3_bucket}/{project_name}"

          logging.info("Initializing inference service specification...")
          resources_spec = client.V1ResourceRequirements(
              requests={"cpu": "1000m", "memory": "8Gi"},
              limits={"cpu": "2000m", "memory": "16Gi"},
          )

          # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1TritonSpec/
          triton_spec = V1beta1TritonSpec(
              args=["--strict-model-config=false"],
              runtime_version="22.03-py3",
              storage_uri=storage_uri,
              resources=resources_spec,
          )

          # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1PredictorSpec/
          predictor_spec = V1beta1PredictorSpec(
              service_account_name="kserve-inference-sa", triton=triton_spec
          )

          if explainer_type:
              print("Found an explainer, which will be co-deployed.")
              # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1AlibiExplainerSpec/
              alibi_spec = V1beta1AlibiExplainerSpec(
                  type=explainer_type,
                  storage_uri=f"{storage_uri}/explainer/{model_version}",  # /explainer.alibi",
                  resources=resources_spec,
              )

              # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1ExplainerSpec/
              explainer_spec = V1beta1ExplainerSpec(
                  min_replicas=1,
                  alibi=alibi_spec,
              )

          # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1InferenceServiceSpec/#properties
          inference_service_spec = V1beta1InferenceService(
              api_version=api_version,
              kind=constants.KSERVE_KIND,
              metadata=client.V1ObjectMeta(
                  name=project_name,
                  namespace=namespace,
                  annotations={"sidecar.istio.io/inject": "false"},
              ),
              spec=V1beta1InferenceServiceSpec(
                  predictor=predictor_spec,
                  explainer=explainer_spec if explainer_type else None,
              ),
          )

          logging.info("Creating inference service...")
          kserve_client = KServeClient()
          kserve_client.create(inference_service_spec)

          logging.info("Waiting for inference service to start...")
          kserve_client.get(
              project_name, namespace=namespace, watch=True, timeout_seconds=180
          )

          logging.info("Getting inference URL...")
          inference_response = kserve_client.get(project_name, namespace=namespace)
          inference_url = inference_response["status"]["address"]["url"]
          logging.info(f"inference URL: {inference_url}")

          logging.info("Finished.")
          return inference_url

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                  str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Deploy model with kserve', description='Deploys a model using KServe and Trino as backend.')
      _parser.add_argument("--project-name", dest="project_name", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--model-version", dest="model_version", type=int, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--explainer-type", dest="explainer_type", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--kserve-version", dest="kserve_version", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--s3-bucket", dest="s3_bucket", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = deploy_model_with_kserve(**_parsed_args)

      _outputs = [_outputs]

      _output_serializers = [
          _serialize_str,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --project-name
    - {inputValue: project_name}
    - --model-version
    - {inputValue: model_version}
    - if:
        cond: {isPresent: explainer_type}
        then:
        - --explainer-type
        - {inputValue: explainer_type}
    - if:
        cond: {isPresent: kserve_version}
        then:
        - --kserve-version
        - {inputValue: kserve_version}
    - if:
        cond: {isPresent: s3_bucket}
        then:
        - --s3-bucket
        - {inputValue: s3_bucket}
    - '----output-paths'
    - {outputPath: Output}
