{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f20b995-81e6-46ae-952a-f36267ee6cea",
   "metadata": {},
   "source": [
    "# Deploy Model with KServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266a528d-a77c-460d-8b4c-3632170c702c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "from kfp.components import create_component_from_func\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "# BASE_IMAGE = \"quay.io/ibm/kubeflow-notebook-image-ppc64le:latest\"\n",
    "BASE_IMAGE = (\n",
    "    \"quay.io/ibm/kubeflow-notebook-image-ppc64le:elyra3.15.0-py3.9-tf2.12.0-pt2.0.1-v2\"\n",
    ")\n",
    "\n",
    "\n",
    "def deploy_model_with_kserve(\n",
    "    project_name: str,\n",
    "    model_version: int,\n",
    "    explainer_type: str = None,\n",
    "    kserve_version: str = \"v1beta1\",\n",
    "    s3_bucket: str = \"projects\",\n",
    "    storage_uri: str = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Deploys a model using KServe and Trino as backend.\n",
    "\n",
    "            Parameters:\n",
    "                    project_name: Name of the project. Must be unique for the targeted namespace and conform Kubernetes naming conventions. Example: my-model.\n",
    "                    explainer_type: Type of Alibi explanation. If None, explanations are not provided. Example: AnchorTabular.\n",
    "                    kserve_version: KServe API version. Example: v1beta1.\n",
    "                    model_version: Version of the deployed model. Relevant to match explainer version to model version. Example: 1.\n",
    "                    s3_bucket: Name of the s3 bucket in which model projects reside. Example: projects.\n",
    "                    storage_uri: Optional full storage URI, overriding dynamic URI creating from project_name. Example: gs://kfserving-examples/models/sklearn/1.0/model.\n",
    "            Returns:\n",
    "                    endpoint: REST endpoint where the model can be queried. Example: https://my-model-user-example-com.apps.myorg.com.\n",
    "    \"\"\"\n",
    "    from kubernetes import client, config\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1AlibiExplainerSpec\n",
    "    from kserve import V1beta1ExplainerSpec\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TritonSpec\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stdout,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(levelname)s %(asctime)s: %(message)s\",\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model_version = int(model_version)\n",
    "    except ValueError:\n",
    "        logging.warning(\n",
    "            \"Could not parse model version. Continuing with default value 1...\"\n",
    "        )\n",
    "        model_version = 1\n",
    "\n",
    "    # See: https://www.kubeflow.org/docs/external-add-ons/kserve/first_isvc_kserve/\n",
    "    logging.info(\"Initializing environment...\")\n",
    "    config.load_incluster_config()\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + kserve_version\n",
    "    if storage_uri is None:\n",
    "        storage_uri: str = f\"s3://{s3_bucket}/{project_name}\"\n",
    "\n",
    "    logging.info(\"Initializing inference service specification...\")\n",
    "    resources_spec = client.V1ResourceRequirements(\n",
    "        requests={\"cpu\": \"1000m\", \"memory\": \"8Gi\"},\n",
    "        limits={\"cpu\": \"2000m\", \"memory\": \"16Gi\"},\n",
    "    )\n",
    "\n",
    "    # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1TritonSpec/\n",
    "    triton_spec = V1beta1TritonSpec(\n",
    "        args=[\"--strict-model-config=false\"],\n",
    "        runtime_version=\"22.03-py3\",\n",
    "        storage_uri=storage_uri,\n",
    "        resources=resources_spec,\n",
    "    )\n",
    "\n",
    "    # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1PredictorSpec/\n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        service_account_name=\"kserve-inference-sa\", triton=triton_spec\n",
    "    )\n",
    "\n",
    "    if explainer_type:\n",
    "        print(\"Found an explainer, which will be co-deployed.\")\n",
    "        # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1AlibiExplainerSpec/\n",
    "        alibi_spec = V1beta1AlibiExplainerSpec(\n",
    "            type=explainer_type,\n",
    "            storage_uri=f\"{storage_uri}/explainer/{model_version}\",  # /explainer.alibi\",\n",
    "            resources=resources_spec,\n",
    "        )\n",
    "\n",
    "        # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1ExplainerSpec/\n",
    "        explainer_spec = V1beta1ExplainerSpec(\n",
    "            min_replicas=1,\n",
    "            alibi=alibi_spec,\n",
    "        )\n",
    "\n",
    "    # See: https://kserve.github.io/website/master/sdk_docs/docs/V1beta1InferenceServiceSpec/#properties\n",
    "    inference_service_spec = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=project_name,\n",
    "            namespace=namespace,\n",
    "            annotations={\"sidecar.istio.io/inject\": \"false\"},\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=predictor_spec,\n",
    "            explainer=explainer_spec if explainer_type else None,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    kserve_client = KServeClient()\n",
    "\n",
    "    logging.info(\"Checking for existing inference service...\")\n",
    "    try:\n",
    "        inference_service = kserve_client.get(project_name, namespace=namespace)\n",
    "        logging.info(f\"Received: {inference_service}\")\n",
    "\n",
    "        if \"status\" in inference_service:\n",
    "            logging.info(\"Inference service already exists.\")\n",
    "\n",
    "            logging.info(\"Patching inference service with new model version...\")\n",
    "            kserve_client.patch(project_name, inference_service_spec)\n",
    "        else:\n",
    "            logging.info(\"Creating inference service...\")\n",
    "            kserve_client.create(inference_service_spec)\n",
    "    except Exception:\n",
    "        logging.info(\"Creating new inference service...\")\n",
    "        kserve_client.create(inference_service_spec)\n",
    "\n",
    "    logging.info(\"Waiting for inference service to start...\")\n",
    "    kserve_client.get(\n",
    "        project_name, namespace=namespace, watch=True, timeout_seconds=180\n",
    "    )\n",
    "\n",
    "    logging.info(\"Getting inference URL...\")\n",
    "    inference_response = kserve_client.get(project_name, namespace=namespace)\n",
    "    inference_url = inference_response[\"status\"][\"address\"][\"url\"]\n",
    "    logging.info(f\"inference URL: {inference_url}\")\n",
    "\n",
    "    logging.info(\"Finished.\")\n",
    "    return inference_url\n",
    "\n",
    "\n",
    "deploy_model_with_kserve_comp = create_component_from_func(\n",
    "    func=deploy_model_with_kserve,\n",
    "    output_component_file=\"component.yaml\",\n",
    "    base_image=BASE_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a736bb-00e2-4d63-afe9-5e932bd122b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
